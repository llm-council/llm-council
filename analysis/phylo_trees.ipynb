{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code ported from @sam_paech\n",
    "# Inspired by: https://x.com/sam_paech/status/1907619496552189991\n",
    "# metrics.py\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load CMU Pronouncing Dictionary\n",
    "pronunciation_dict = cmudict.dict()\n",
    "\n",
    "def syllable_count(word):\n",
    "    \"\"\"Determine the number of syllables in a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    if word in pronunciation_dict:\n",
    "        return max([len([phoneme for phoneme in phonetic if phoneme[-1].isdigit()]) for phonetic in pronunciation_dict[word]])\n",
    "    return 1  # Assume one syllable if the word isn't found\n",
    "\n",
    "def is_polysyllabic(word):\n",
    "    \"\"\"Identify if a word is polysyllabic (i.e., has 3 or more syllables).\"\"\"\n",
    "    return syllable_count(word) >= 3\n",
    "\n",
    "def calculate_complexity_index(text):\n",
    "    \"\"\"\n",
    "    Calculate a complexity index (0-100) based on Flesch-Kincaid grade level and percentage of complex words.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        float: Complexity index from 0-100\n",
    "    \"\"\"\n",
    "    # Handle empty text\n",
    "    if not text or not text.strip():\n",
    "        return 0\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    sentence_count = max(1, len(sentences))\n",
    "    word_count = max(1, len(tokens))\n",
    "    \n",
    "    # Calculate Flesch-Kincaid Grade Level using the same formula as the reference code\n",
    "    total_syllables = sum(syllable_count(token) for token in tokens)\n",
    "    fk_grade_level = 0.39 * (word_count / sentence_count) + 11.8 * (total_syllables / word_count) - 15.59\n",
    "    \n",
    "    # Calculate percentage of complex words\n",
    "    complex_word_count = sum(1 for token in tokens if is_polysyllabic(token))\n",
    "    percent_complex_words = (complex_word_count / word_count) * 100\n",
    "    \n",
    "    # Cap FK grade at 14 (college level)\n",
    "    fk_grade_level = min(fk_grade_level, 14)\n",
    "    \n",
    "    # Cap percent complex at 20%\n",
    "    percent_complex_words = min(percent_complex_words, 20)\n",
    "    \n",
    "    # Normalize scores to 0-100 range\n",
    "    fk_normalized = (fk_grade_level / 14) * 100\n",
    "    complex_normalized = (percent_complex_words / 20) * 100\n",
    "    \n",
    "    # Average the two normalized scores for the final complexity index\n",
    "    complexity_index = (fk_normalized + complex_normalized) / 2\n",
    "    \n",
    "    return round(complexity_index, 2)\n",
    "\n",
    "# Calculates a slop score for a provided text\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def load_and_preprocess_slop_words():\n",
    "    with open('data/slop_phrase_prob_adjustments.json', 'r') as f:\n",
    "        slop_phrases = json.load(f)\n",
    "    \n",
    "    phrase_weighting = [1.0 - prob_adjustment for word, prob_adjustment in slop_phrases]\n",
    "    max_score = max(phrase_weighting)\n",
    "    scaled_weightings = [score / max_score for score in phrase_weighting]\n",
    "    n_slop_words = 600\n",
    "    return {word.lower(): score for (word, _), score in zip(slop_phrases[:n_slop_words], scaled_weightings[:n_slop_words])}\n",
    "\n",
    "def extract_text_blocks(file_path, compiled_pattern):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    matches = compiled_pattern.findall(content)\n",
    "    return '\\n'.join(matches)\n",
    "\n",
    "def calculate_slop_score_chunk(args):\n",
    "    text, slop_words_chunk = args\n",
    "    return sum(\n",
    "        score * len(re.findall(r'\\b' + re.escape(word) + r'\\b', text))\n",
    "        for word, score in slop_words_chunk.items()\n",
    "    )\n",
    "\n",
    "def split_into_chunks(slop_words, num_chunks):\n",
    "    slop_words_items = list(slop_words.items())\n",
    "    chunk_size = len(slop_words_items) // num_chunks\n",
    "    if chunk_size == 0:\n",
    "        chunk_size = 1\n",
    "    return [dict(slop_words_items[i:i + chunk_size]) for i in range(0, len(slop_words_items), chunk_size)]\n",
    "\n",
    "\n",
    "# Call this to function to calculate a slop score.\n",
    "# This is the way it's calculated for the eqbench creative writing leaderboard.\n",
    "def calculate_slop_index(extracted_text):    \n",
    "    slop_words = load_and_preprocess_slop_words()\n",
    "    \n",
    "    num_chunks = 12 #mp.cpu_count()\n",
    "    slop_words_chunks = split_into_chunks(slop_words, num_chunks)\n",
    "    \n",
    "    if not extracted_text:\n",
    "        slop_index = 0.0\n",
    "    else:\n",
    "        # Parallelize the calculation using joblib\n",
    "        slop_scores = Parallel(n_jobs=num_chunks)(delayed(calculate_slop_score_chunk)((extracted_text, chunk)) for chunk in slop_words_chunks)\n",
    "        \n",
    "        slop_score = sum(slop_scores)\n",
    "        total_words = len(extracted_text.split())\n",
    "        slop_index = (slop_score / total_words) * 1000 if total_words > 0 else 0\n",
    "    return slop_index\n",
    "\n",
    "\n",
    "import re\n",
    "import json # Keep if you might load texts from JSON elsewhere\n",
    "from collections import Counter, defaultdict # Added defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from wordfreq import word_frequency\n",
    "import os # Keep if needed elsewhere\n",
    "import string # Keep if needed elsewhere\n",
    "from typing import List, Tuple, Dict, Set # Added type hints\n",
    "\n",
    "# --- BEGIN HELPER FUNCTIONS (Verified & Slightly Corrected) ---\n",
    "\n",
    "# Define the set of forbidden substrings (lowercase)\n",
    "FORBIDDEN_SUBSTRINGS = {\n",
    "    \"jolyne\", \"yennefer\", \"revy\", \"stoer\", \"beretta\", \"sisyphus\",\n",
    "    \"retiarius\", \"alucard\", \"azra\", \"jigen\", \"ludus\", \"professore\",\n",
    "    \"alessandra\", \"midas\", \"chewy\",\n",
    "    \"makima\", \"neegan\", \"immateria\", \"offworlder\", \"piguaquan\",\n",
    "    \"vengerberg\", \"lanista\", \"morska\", \"scythan\", \"woolong\",\n",
    "    \"cujoh\", \"underhold\",\n",
    "    \"darkroom\", \"bookstore\", \"guildmaster\", \"volkov\", \"katra\", \"arthur\",\n",
    "    \"lucifer\", \"lilith\", \"antares\", \"chronowatch\", \"nettle\", \"nettes\",\n",
    "    \"busker\", \"rewound\", \"rewind\", \"laddie\",\n",
    "    \"spike\", \"elliot\", \"vespa\", \"alasdair\", \"sorceress\", \"mora\",\n",
    "    \"lighthouse\", \"gladiator\", \"bookshop\", \"koala\", \"crow\", \"boulder\",\n",
    "    \"jt\", \"interstellar\", \"dreamscape\", \"xxxx\"\n",
    "}\n",
    "\n",
    "\n",
    "# Regex pattern for word extraction (compile once)\n",
    "WORD_PATTERN = re.compile(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\")\n",
    "\n",
    "def normalize_apostrophes(text):\n",
    "    \"\"\"Replaces common apostrophe variants with the standard ASCII apostrophe.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.replace(\"‘\", \"'\")\n",
    "    text = text.replace(\"ʼ\", \"'\")\n",
    "    return text\n",
    "\n",
    "def _extract_words(normalized_text: str, min_length: int = 4) -> List[str]:\n",
    "    \"\"\"Extracts words meeting criteria from normalized text.\"\"\"\n",
    "    words = WORD_PATTERN.findall(normalized_text)\n",
    "    return [\n",
    "        word for word in words\n",
    "        if len(word) >= min_length or \"'\" in word\n",
    "    ]\n",
    "\n",
    "def get_word_counts(texts: List[str], min_length: int = 4) -> Counter:\n",
    "    \"\"\"\n",
    "    Count overall word frequencies in a list of texts.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for text in tqdm(texts, desc=\"Counting words\", leave=False, disable=True): # Often nested, disable bar\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "        normalized_text = normalize_apostrophes(text.lower())\n",
    "        words = _extract_words(normalized_text, min_length)\n",
    "        word_counts.update(words)\n",
    "    return word_counts\n",
    "\n",
    "# --- NEW HELPER FUNCTION ---\n",
    "def get_word_prompt_map(texts_with_ids: List[Tuple[str, str]], min_length: int = 4) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Creates a map of words to the set of prompt IDs they appear in.\n",
    "    \"\"\"\n",
    "    word_prompts = defaultdict(set)\n",
    "    for text, prompt_id in tqdm(texts_with_ids, desc=\"Mapping words to prompts\", leave=False, disable=True): # Often nested\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "        normalized_text = normalize_apostrophes(text.lower())\n",
    "        words = _extract_words(normalized_text, min_length)\n",
    "        for word in words:\n",
    "            word_prompts[word].add(prompt_id)\n",
    "    return dict(word_prompts) # Convert back to standard dict if preferred\n",
    "\n",
    "# --- EXISTING HELPER FUNCTIONS (Unchanged, use original code) ---\n",
    "def filter_mostly_numeric(word_counts):\n",
    "    \"\"\"Filters out words containing a high proportion of digits.\"\"\"\n",
    "    def is_mostly_numbers(word):\n",
    "        if not word: return False\n",
    "        digit_count = sum(c.isdigit() for c in word)\n",
    "        return (digit_count / len(word) > 0.2) if len(word) > 0 else False\n",
    "\n",
    "    return Counter({word: count for word, count in word_counts.items() if not is_mostly_numbers(word)})\n",
    "\n",
    "KNOWN_CONTRACTIONS_S = {\n",
    "    \"it's\", \"that's\", \"what's\", \"who's\", \"he's\", \"she's\",\n",
    "    \"there's\", \"here's\", \"where's\", \"when's\", \"why's\", \"how's\",\n",
    "    \"let's\"\n",
    "}\n",
    "\n",
    "def merge_plural_possessive_s(word_counts):\n",
    "    \"\"\"Merges counts of possessive words ending in 's with their base words, excluding known contractions.\"\"\"\n",
    "    merged_counts = Counter()\n",
    "    for word, count in word_counts.items(): # No tqdm needed here, usually fast\n",
    "        if word.endswith(\"'s\") and word not in KNOWN_CONTRACTIONS_S:\n",
    "            base_word = word[:-2]\n",
    "            if base_word:\n",
    "                merged_counts[base_word] += count\n",
    "        else:\n",
    "            merged_counts[word] += count\n",
    "    return merged_counts\n",
    "\n",
    "def filter_forbidden_words(word_counts, forbidden_substrings):\n",
    "    \"\"\"Filters out words containing any of the forbidden substrings (case-insensitive).\"\"\"\n",
    "    if not forbidden_substrings:\n",
    "        return word_counts\n",
    "    return Counter({\n",
    "        word: count for word, count in word_counts.items()\n",
    "        if not any(sub in word.lower() for sub in forbidden_substrings)\n",
    "    })\n",
    "\n",
    "def filter_by_minimum_count(word_counts, min_count):\n",
    "    \"\"\"Filters out words that appear less than or equal to min_count times.\"\"\"\n",
    "    if min_count <= 0:\n",
    "        return word_counts\n",
    "    return Counter({word: count for word, count in word_counts.items() if count > min_count})\n",
    "\n",
    "def analyze_word_rarity(word_counts):\n",
    "    \"\"\"Analyzes word rarity based on corpus and wordfreq frequencies.\"\"\"\n",
    "    if not word_counts: return {}, {}, np.nan, np.nan, np.nan\n",
    "    total_words = sum(word_counts.values())\n",
    "    if total_words == 0: return {}, {}, np.nan, np.nan, np.nan\n",
    "\n",
    "    corpus_frequencies = {word: count / total_words for word, count in word_counts.items()}\n",
    "    wordfreq_frequencies = {}\n",
    "    for word in tqdm(list(word_counts.keys()), desc=\"Fetching wordfreq data\", leave=False, disable=True): # Often nested\n",
    "        wordfreq_frequencies[word] = word_frequency(word, 'en')\n",
    "\n",
    "    valid_words = [word for word, freq in wordfreq_frequencies.items() if freq > 0]\n",
    "    if not valid_words: return corpus_frequencies, wordfreq_frequencies, np.nan, np.nan, np.nan\n",
    "\n",
    "    corpus_freq_list = [corpus_frequencies[word] for word in valid_words]\n",
    "    wordfreq_freq_list = [wordfreq_frequencies[word] for word in valid_words]\n",
    "\n",
    "    avg_corpus_rarity = np.mean([-np.log10(freq) for freq in corpus_freq_list]) if corpus_freq_list else np.nan\n",
    "    avg_wordfreq_rarity = np.mean([-np.log10(freq) for freq in wordfreq_freq_list]) if wordfreq_freq_list else np.nan\n",
    "\n",
    "    correlation = np.nan\n",
    "    if len(corpus_freq_list) >= 2:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'): # Suppress warnings for log10(0) or corrcoef issues\n",
    "            correlation_matrix = np.corrcoef(corpus_freq_list, wordfreq_freq_list)\n",
    "            if isinstance(correlation_matrix, np.ndarray) and correlation_matrix.shape == (2, 2):\n",
    "                 correlation = correlation_matrix[0, 1]\n",
    "\n",
    "    return corpus_frequencies, wordfreq_frequencies, avg_corpus_rarity, avg_wordfreq_rarity, correlation\n",
    "\n",
    "\n",
    "def find_over_represented_words(corpus_frequencies, wordfreq_frequencies, top_n=50000):\n",
    "    \"\"\"Finds words most over-represented compared to wordfreq.\"\"\"\n",
    "    over_representation = {}\n",
    "    for word, corpus_freq in corpus_frequencies.items():\n",
    "        wordfreq_freq = wordfreq_frequencies.get(word, 0)\n",
    "        if wordfreq_freq > 0:\n",
    "            over_representation[word] = corpus_freq / wordfreq_freq\n",
    "        elif corpus_freq > 0:\n",
    "             over_representation[word] = corpus_freq / 1e-12\n",
    "\n",
    "    return sorted(over_representation.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "\n",
    "\n",
    "# --- MODIFIED MAIN FUNCTIONS ---\n",
    "\n",
    "def _get_filtered_word_counts(\n",
    "    texts_with_ids: List[Tuple[str, str]],\n",
    "    min_repetition_count: int,\n",
    "    min_prompt_ids: int = 2 # New parameter: Minimum number of unique prompt IDs a word must appear in\n",
    ") -> Counter:\n",
    "    \"\"\"\n",
    "    Internal helper to get word counts filtered by numeric, possessive,\n",
    "    forbidden, minimum prompts, and minimum overall count.\n",
    "    \"\"\"\n",
    "    if not texts_with_ids:\n",
    "        return Counter()\n",
    "\n",
    "    # Extract all texts for overall counting\n",
    "    all_texts = [text for text, _ in texts_with_ids]\n",
    "    if not all_texts:\n",
    "        return Counter()\n",
    "\n",
    "    # 1. Get overall raw counts\n",
    "    raw_word_counts = get_word_counts(all_texts)\n",
    "    # 2. Filter numeric words\n",
    "    filtered_counts_numeric = filter_mostly_numeric(raw_word_counts)\n",
    "    # 3. Merge possessives\n",
    "    merged_counts = merge_plural_possessive_s(filtered_counts_numeric)\n",
    "    # 4. Filter forbidden words\n",
    "    filtered_counts_forbidden = filter_forbidden_words(merged_counts, FORBIDDEN_SUBSTRINGS)\n",
    "\n",
    "    # *** NEW: Filter by minimum prompt IDs ***\n",
    "    if min_prompt_ids > 1:\n",
    "        # Get the map of word -> set(prompt_ids)\n",
    "        word_prompt_map = get_word_prompt_map(texts_with_ids)\n",
    "        # Identify words appearing in enough distinct prompts\n",
    "        multi_prompt_words = {\n",
    "            word for word, prompt_ids in word_prompt_map.items()\n",
    "            if len(prompt_ids) >= min_prompt_ids\n",
    "        }\n",
    "        # Filter the counts to keep only multi-prompt words\n",
    "        filtered_counts_multi_prompt = Counter({\n",
    "            word: count for word, count in filtered_counts_forbidden.items()\n",
    "            if word in multi_prompt_words\n",
    "        })\n",
    "    else:\n",
    "        # If min_prompt_ids is 1 or less, skip this filtering step\n",
    "        filtered_counts_multi_prompt = filtered_counts_forbidden\n",
    "\n",
    "\n",
    "    # 5. Filter by minimum overall repetition count (applied AFTER multi-prompt filter)\n",
    "    final_counts = filter_by_minimum_count(filtered_counts_multi_prompt, min_repetition_count)\n",
    "\n",
    "    return final_counts\n",
    "\n",
    "\n",
    "def calculate_repetition_metric(\n",
    "    texts_with_ids: List[Tuple[str, str]],\n",
    "    top_n: int = 100,\n",
    "    min_repetition_count: int = 5,\n",
    "    min_prompt_ids: int = 2 # Minimum number of unique prompt IDs a word must appear in\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate a repetition metric based on over-represented words,\n",
    "    filtering out forbidden words, words occurring infrequently overall,\n",
    "    and words not appearing in at least `min_prompt_ids` unique prompts.\n",
    "\n",
    "    Args:\n",
    "        texts_with_ids: List of tuples (text_sample, prompt_id) to analyze.\n",
    "        top_n: Number of top over-represented words to consider for the score.\n",
    "        min_repetition_count: Minimum number of times a word must appear in the\n",
    "                              *entire* corpus (across all prompts) to be considered.\n",
    "                              Words appearing `min_repetition_count` or fewer\n",
    "                              times are excluded *after* multi-prompt filtering.\n",
    "        min_prompt_ids: Minimum number of unique prompt IDs a word must have\n",
    "                        appeared in to be considered for the analysis. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        float: Repetition score (sum of corpus frequencies of top_n\n",
    "               over-represented words, as a percentage), considering only words\n",
    "               meeting all filtering criteria.\n",
    "    \"\"\"\n",
    "    final_counts = _get_filtered_word_counts(texts_with_ids, min_repetition_count, min_prompt_ids)\n",
    "\n",
    "    if not final_counts:\n",
    "        print(f\"Warning: No words remaining after filtering (min count > {min_repetition_count}, min prompts >= {min_prompt_ids}).\")\n",
    "        return 0.0\n",
    "\n",
    "    # Analyze rarity (uses the final filtered counts)\n",
    "    corpus_frequencies, wordfreq_frequencies, _, _, _ = analyze_word_rarity(final_counts)\n",
    "\n",
    "    if not corpus_frequencies:\n",
    "        print(\"Warning: Corpus frequencies could not be calculated (possibly all remaining words have zero wordfreq).\")\n",
    "        return 0.0\n",
    "\n",
    "    # Find over-represented words (based on final filtered data)\n",
    "    over_represented = find_over_represented_words(corpus_frequencies, wordfreq_frequencies, top_n=top_n)\n",
    "\n",
    "    # Calculate score: Sum the corpus frequencies of these top words\n",
    "    repetition_score = sum(corpus_frequencies.get(word, 0) for word, score in over_represented)\n",
    "\n",
    "    # Normalize to percentage\n",
    "    normalized_score = repetition_score * 100\n",
    "\n",
    "    return round(normalized_score, 4)\n",
    "\n",
    "\n",
    "def get_top_repetitive_words(\n",
    "    texts_with_ids: List[Tuple[str, str]],\n",
    "    top_n: int = 20,\n",
    "    min_repetition_count: int = 5,\n",
    "    min_prompt_ids: int = 2 # Minimum number of unique prompt IDs a word must appear in\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Get the top over-represented words with their scores, filtering out\n",
    "    forbidden words, words occurring infrequently overall, and words not\n",
    "    appearing in at least `min_prompt_ids` unique prompts.\n",
    "\n",
    "    Args:\n",
    "        texts_with_ids: List of tuples (text_sample, prompt_id) to analyze.\n",
    "        top_n: Number of top over-represented words to return.\n",
    "        min_repetition_count: Minimum number of times a word must appear in the\n",
    "                              *entire* corpus (across all prompts) to be considered.\n",
    "                              Words appearing `min_repetition_count` or fewer\n",
    "                              times are excluded *after* multi-prompt filtering.\n",
    "        min_prompt_ids: Minimum number of unique prompt IDs a word must have\n",
    "                        appeared in to be considered for the analysis. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples (word, over_representation_score) for words meeting\n",
    "        all filtering criteria.\n",
    "    \"\"\"\n",
    "    final_counts = _get_filtered_word_counts(texts_with_ids, min_repetition_count, min_prompt_ids)\n",
    "\n",
    "    if not final_counts:\n",
    "        print(f\"Warning: No words remaining after filtering (min count > {min_repetition_count}, min prompts >= {min_prompt_ids}).\")\n",
    "        return []\n",
    "\n",
    "    # Analyze rarity\n",
    "    corpus_frequencies, wordfreq_frequencies, _, _, _ = analyze_word_rarity(final_counts)\n",
    "\n",
    "    if not corpus_frequencies:\n",
    "         print(\"Warning: Corpus frequencies could not be calculated (possibly all remaining words have zero wordfreq).\")\n",
    "         return []\n",
    "\n",
    "    # Find over-represented words\n",
    "    over_represented = find_over_represented_words(corpus_frequencies, wordfreq_frequencies, top_n=top_n)\n",
    "\n",
    "    # Return words and their *over-representation* score, not frequency\n",
    "    return over_represented\n",
    "\n",
    "\n",
    "\n",
    "# --- Add these imports ---\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams as nltk_ngrams # Alias to avoid potential name conflict\n",
    "import string\n",
    "from collections import Counter, defaultdict # Counter/defaultdict likely already there\n",
    "\n",
    "# --- Add NLTK downloads (run once) ---\n",
    "if False:\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except nltk.downloader.DownloadError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except nltk.downloader.DownloadError:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# --- Initialize stopwords and punctuation (global scope is fine here) ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_set = set(string.punctuation) # Use a different name than the module\n",
    "\n",
    "def get_multi_prompt_ngrams(\n",
    "    prompts_data: Dict[str, List[str]],\n",
    "    n: int,\n",
    "    top_k: int = 20,\n",
    "    min_prompt_ids: int = 2\n",
    ") -> List[Tuple[Tuple[str, ...], int]]:\n",
    "    \"\"\"\n",
    "    Extracts the top_k N-grams of size n that appear across at least\n",
    "    min_prompt_ids unique prompts within the provided texts.\n",
    "\n",
    "    Args:\n",
    "        prompts_data: Dict mapping prompt_id to a list of text responses.\n",
    "                      Example: { 'prompt1': ['text a', 'text b'], 'prompt2': ['text c'] }\n",
    "        n: The size of the N-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "        top_k: The maximum number of top N-grams to return.\n",
    "        min_prompt_ids: The minimum number of unique prompt IDs an N-gram\n",
    "                        must appear in to be considered.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the N-gram (as a tuple of strings)\n",
    "        and its total frequency, sorted by frequency descending.\n",
    "        Returns an empty list if no N-grams meet the criteria.\n",
    "    \"\"\"\n",
    "    ngram_counts = Counter()\n",
    "    ngram_prompt_map = defaultdict(set)\n",
    "\n",
    "    # Check if there are enough unique prompts to even potentially meet the criteria\n",
    "    if len(prompts_data) < min_prompt_ids:\n",
    "        return []\n",
    "\n",
    "    print(f\"      Extracting {n}-grams...\", end=\"\") # Progress indicator within model processing\n",
    "    total_processed_texts = 0\n",
    "    # 1. Iterate through texts, extract cleaned n-grams, count, and map to prompts\n",
    "    for prompt_id, texts in prompts_data.items():\n",
    "        for text in texts:\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                continue\n",
    "            total_processed_texts += 1\n",
    "            # Tokenize and clean\n",
    "            tokens = [\n",
    "                word.lower() for word in word_tokenize(text)\n",
    "                if word.isalpha() and word.lower() not in stop_words # Keep alpha, remove stops\n",
    "            ]\n",
    "\n",
    "            if len(tokens) < n:\n",
    "                continue # Not enough tokens to form an n-gram\n",
    "\n",
    "            # Generate n-grams\n",
    "            current_ngrams = list(nltk_ngrams(tokens, n))\n",
    "\n",
    "            # Update counts and prompt map\n",
    "            for ngram in current_ngrams:\n",
    "                ngram_counts[ngram] += 1\n",
    "                ngram_prompt_map[ngram].add(prompt_id)\n",
    "\n",
    "    print(f\" Done ({total_processed_texts} texts).\") # Finish progress indicator\n",
    "\n",
    "    # 2. Filter n-grams based on min_prompt_ids\n",
    "    filtered_ngrams = {\n",
    "        ngram: count for ngram, count in ngram_counts.items()\n",
    "        if len(ngram_prompt_map[ngram]) >= min_prompt_ids\n",
    "    }\n",
    "\n",
    "    if not filtered_ngrams:\n",
    "        return []\n",
    "\n",
    "    # 3. Sort the filtered n-grams by frequency and return top_k\n",
    "    # Create a list of (ngram, frequency) tuples from the filtered dict\n",
    "    sorted_filtered_ngrams = sorted(filtered_ngrams.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    return sorted_filtered_ngrams[:top_k]\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os # For checking file existence\n",
    "from collections import Counter # Useful for counting n-grams in text\n",
    "\n",
    "# Assume NLTK is available and imported if needed for tokenization/ngrams\n",
    "# If not using NLTK, adjust tokenization/ngram generation accordingly.\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk import ngrams\n",
    "    nltk_available = True\n",
    "    # Minimal check for resources needed\n",
    "    if False:\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt', quiet=True)\n",
    "        except:\n",
    "            print(\"Warning: NLTK 'punkt' resource might be needed. Downloading...\")\n",
    "            nltk.download('punkt', quiet=True)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Warning: NLTK not installed. Using basic split() for tokenization.\")\n",
    "    nltk_available = False\n",
    "\n",
    "# --- Helper Function to Load Slop Lists ---\n",
    "\n",
    "def load_slop_list_to_set(filename):\n",
    "    \"\"\"Loads slop words/phrases from the specific JSON format into a set.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Warning: Slop file not found: {filename}. Returning empty set.\")\n",
    "        return set()\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        # Extract the first element from each inner list and lowercase it\n",
    "        # Handles format like [[\"word1\"], [\"word2 phrase\"], ...]\n",
    "        slop_items = {item[0].lower() for item in data if item} # Ensure inner list is not empty\n",
    "        print(f\"Loaded {len(slop_items)} items from {filename}\")\n",
    "        return slop_items\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {filename}. Returning empty set.\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}. Returning empty set.\")\n",
    "        return set()\n",
    "\n",
    "# --- New Slop Index Calculation Function ---\n",
    "\n",
    "def calculate_slop_index_new(extracted_text, debug=True):\n",
    "    \"\"\"\n",
    "    Calculates a slop index based on hits in word, bigram, and trigram slop lists.\n",
    "\n",
    "    Args:\n",
    "        extracted_text (str): The text to analyze.\n",
    "        debug (bool): If True, prints the hit counts for each list.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated slop index.\n",
    "    \"\"\"\n",
    "    # 1. Load Slop Lists\n",
    "    slop_words_set = load_slop_list_to_set('data/slop_list.json')\n",
    "    slop_bigrams_set = load_slop_list_to_set('data/slop_list_bigrams.json')\n",
    "    slop_trigrams_set = load_slop_list_to_set('data/slop_list_trigrams.json')\n",
    "\n",
    "    # Check if any lists were loaded\n",
    "    if not slop_words_set and not slop_bigrams_set and not slop_trigrams_set:\n",
    "        print(\"Error: No slop lists could be loaded. Returning slop index 0.\")\n",
    "        return 0.0\n",
    "\n",
    "    if not extracted_text or not isinstance(extracted_text, str):\n",
    "        if debug:\n",
    "            print(\"Input text is empty or invalid.\")\n",
    "            print(f\"Word Hits: 0\")\n",
    "            print(f\"Bigram Hits: 0\")\n",
    "            print(f\"Trigram Hits: 0\")\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Preprocess Text and Count Total Words\n",
    "    lower_text = extracted_text.lower()\n",
    "    # Use NLTK tokenizer if available for better handling of punctuation,\n",
    "    # otherwise use a simple regex split for words.\n",
    "    if nltk_available:\n",
    "        tokens = [token for token in word_tokenize(lower_text) if token.isalnum()] # Keep alphanumeric\n",
    "    else:\n",
    "        tokens = re.findall(r'\\b\\w+\\b', lower_text) # Simple word split\n",
    "\n",
    "    total_words = len(tokens)\n",
    "    if total_words == 0:\n",
    "        if debug:\n",
    "            print(\"No valid words found in the text after tokenization.\")\n",
    "            print(f\"Word Hits: 0\")\n",
    "            print(f\"Bigram Hits: 0\")\n",
    "            print(f\"Trigram Hits: 0\")\n",
    "        return 0.0\n",
    "\n",
    "    # 3. Count Hits\n",
    "    word_hits = 0\n",
    "    bigram_hits = 0\n",
    "    trigram_hits = 0\n",
    "\n",
    "    # Count word hits\n",
    "    if slop_words_set:\n",
    "        word_hits = sum(1 for token in tokens if token in slop_words_set)\n",
    "\n",
    "    # Count bigram hits\n",
    "    if slop_bigrams_set and len(tokens) >= 2:\n",
    "        text_bigrams = ngrams(tokens, 2) if nltk_available else zip(tokens, tokens[1:])\n",
    "        for bigram_tuple in text_bigrams:\n",
    "            bigram_str = ' '.join(bigram_tuple)\n",
    "            if bigram_str in slop_bigrams_set:\n",
    "                bigram_hits += 1\n",
    "\n",
    "    # Count trigram hits\n",
    "    if slop_trigrams_set and len(tokens) >= 3:\n",
    "        text_trigrams = ngrams(tokens, 3) if nltk_available else zip(tokens, tokens[1:], tokens[2:])\n",
    "        for trigram_tuple in text_trigrams:\n",
    "            trigram_str = ' '.join(trigram_tuple)\n",
    "            if trigram_str in slop_trigrams_set:\n",
    "                trigram_hits += 1\n",
    "\n",
    "    # 4. Calculate Final Score\n",
    "    total_slop_score = word_hits + 2*bigram_hits + 8*trigram_hits\n",
    "    # Use the same normalization factor as the original function for consistency\n",
    "    slop_index = (total_slop_score / total_words) * 1000 if total_words > 0 else 0\n",
    "\n",
    "    # 5. Debug Output\n",
    "    if debug:\n",
    "        print(\"--- Slop Index Debug ---\")\n",
    "        print(f\"Total Words Analyzed: {total_words}\")\n",
    "        print(f\"Word Hits: {word_hits} (using {len(slop_words_set)} slop words)\")\n",
    "        print(f\"Bigram Hits: {bigram_hits} (using {len(slop_bigrams_set)} slop bigrams)\")\n",
    "        print(f\"Trigram Hits: {trigram_hits} (using {len(slop_trigrams_set)} slop trigrams)\")\n",
    "        print(f\"Total Hits: {total_slop_score}\")\n",
    "        print(f\"Calculated Slop Index: {slop_index:.4f}\")\n",
    "        print(\"------------------------\")\n",
    "\n",
    "    return slop_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# phylo code\n",
    "\n",
    "import os\n",
    "# Force Qt to run in 'offscreen' mode so ETE won't crash in headless environments\n",
    "os.environ[\"QT_QPA_PLATFORM\"] = \"offscreen\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, to_tree\n",
    "from ete3 import Tree, TreeStyle, NodeStyle, faces, TextFace\n",
    "\n",
    "##############################################################################\n",
    "# 1) Family assignments\n",
    "##############################################################################\n",
    "# Note: You said “gemma & gemini are both google.” So any model name \n",
    "# containing \"gemma\" or \"gemini\" is also mapped to Google in this dict.\n",
    "\n",
    "model_to_family = {\n",
    "    'command-r-plus': 'Cohere',\n",
    "    'command-r': 'Cohere',\n",
    "    'dbrx-instruct': 'DeepSeek',\n",
    "    'qwen1.5-110B-Chat': 'Qwen',\n",
    "    'qwen1.5-72B-Chat': 'Qwen',\n",
    "    'qwen1.5-32B-Chat': 'Qwen',\n",
    "    'llama-3-70b-chat': 'Meta-Llama',\n",
    "    'llama-3-8b-chat': 'Meta-Llama',\n",
    "    'claude-3-opus': 'Anthropic',\n",
    "    'claude-3-sonnet': 'Anthropic',\n",
    "    'claude-3-haiku': 'Anthropic',\n",
    "    'gemini-1.5-pro': 'Google',\n",
    "    'gemini-1.0-pro': 'Google',\n",
    "    'mixtral-8x7b': 'Mistral',\n",
    "    'mistral-medium': 'Mistral',\n",
    "    'mistral-large': 'Mistral',\n",
    "    'gpt-4o-2024-05-13': 'OpenAI',\n",
    "    'gpt-3.5-turbo-0125': 'OpenAI',\n",
    "    'gpt-4-0613': 'OpenAI',\n",
    "    'gpt-4-turbo-2024-04-09': 'OpenAI',\n",
    "}\n",
    "\n",
    "##############################################################################\n",
    "# Family colors to match the above\n",
    "##############################################################################\n",
    "family_colors = {\n",
    "    'Google':     '#8a5cf5',\n",
    "    'Anthropic':  '#ffc13b',\n",
    "    'DeepSeek':   '#1eb980',\n",
    "    'OpenAI':     '#ff5c8d',\n",
    "    'Mistral':    '#ff6e40',\n",
    "    'Meta-Llama': '#1e3d59',\n",
    "    'Qwen':       '#b2df8a',\n",
    "    'Cohere':     '#bebada',\n",
    "    'Sam-Paech':  '#f28e2c',\n",
    "    'Liquid':     '#767676',\n",
    "    'Reka':       '#fb8072',\n",
    "    # If not in dictionary, default to \"Other\"\n",
    "    'Other':      '#cccccc'\n",
    "}\n",
    "\n",
    "##############################################################################\n",
    "# 2) ETE3 Tree Conversion\n",
    "##############################################################################\n",
    "def scipy_cluster_to_ete(scipy_node, ete_parent, id_to_label):\n",
    "    \"\"\"\n",
    "    Recursively convert a scipy.cluster.hierarchy.ClusterNode (from to_tree)\n",
    "    into an ete3 Tree.\n",
    "    \"\"\"\n",
    "    if scipy_node.is_leaf():\n",
    "        leaf_name = id_to_label[scipy_node.id]\n",
    "        ete_parent.name = leaf_name\n",
    "    else:\n",
    "        left_child = ete_parent.add_child()\n",
    "        scipy_cluster_to_ete(scipy_node.left, left_child, id_to_label)\n",
    "\n",
    "        right_child = ete_parent.add_child()\n",
    "        scipy_cluster_to_ete(scipy_node.right, right_child, id_to_label)\n",
    "\n",
    "def linkage_to_ete_tree(linkage_matrix, labels):\n",
    "    \"\"\"\n",
    "    Convert SciPy linkage + labels => ETE3 Tree using to_tree.\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import to_tree\n",
    "    root_node = to_tree(linkage_matrix, rd=False)\n",
    "    ete_root = Tree()\n",
    "    id_to_label = dict(enumerate(labels))\n",
    "    scipy_cluster_to_ete(root_node, ete_root, id_to_label)\n",
    "    return ete_root\n",
    "\n",
    "##############################################################################\n",
    "# 3) ETE3 Rendering Functions\n",
    "##############################################################################\n",
    "def prettify_node(node, label_color=\"black\", circle_color=\"#888888\", circle_size=8):\n",
    "    \"\"\"\n",
    "    For a leaf: add a label face and a colored circle.\n",
    "    For an internal node: no label, but draw thicker lines.\n",
    "    \"\"\"\n",
    "    from ete3 import NodeStyle, TextFace, faces\n",
    "\n",
    "    if node.is_leaf():\n",
    "        tf = TextFace(node.name, fsize=10, fgcolor=label_color)\n",
    "        faces.add_face_to_node(tf, node, column=0, position=\"branch-right\")\n",
    "\n",
    "        style = NodeStyle()\n",
    "        style[\"size\"] = circle_size\n",
    "        style[\"fgcolor\"] = circle_color\n",
    "        style[\"shape\"] = \"circle\"\n",
    "        style[\"hz_line_width\"] = 2\n",
    "        style[\"vt_line_width\"] = 2\n",
    "        node.set_style(style)\n",
    "    else:\n",
    "        style = NodeStyle()\n",
    "        style[\"size\"] = 0  # no circle\n",
    "        style[\"hz_line_width\"] = 2\n",
    "        style[\"vt_line_width\"] = 2\n",
    "        node.set_style(style)\n",
    "\n",
    "def render_ete_tree(\n",
    "    ete_tree,\n",
    "    output_image=\"ete_tree.png\",\n",
    "    layout=\"c\",\n",
    "    model_to_family=None,\n",
    "    family_colors=None,\n",
    "    outdir=\"phylo_trees\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Render the ETE3 tree in either circular ('c') or rectangular ('r') layout.\n",
    "    Apply coloring based on model family.\n",
    "    \"\"\"\n",
    "    from ete3 import TreeStyle\n",
    "\n",
    "    # Default to empty dict if not provided\n",
    "    if model_to_family is None:\n",
    "        model_to_family = {}\n",
    "    if family_colors is None:\n",
    "        family_colors = {}\n",
    "\n",
    "    # Create a TreeStyle\n",
    "    ts = TreeStyle()\n",
    "    ts.mode = layout  # 'c' => circular, 'r' => rectangular\n",
    "    ts.show_leaf_name = False\n",
    "    ts.show_branch_length = False\n",
    "    ts.show_scale = False\n",
    "    # Increase spacing for rectangular trees\n",
    "    if layout == 'r':\n",
    "        ts.branch_vertical_margin = 10\n",
    "\n",
    "    def layout_fn(node):\n",
    "        label = node.name\n",
    "        family = model_to_family.get(label, \"Other\")\n",
    "        circle_color = family_colors.get(family, \"#cccccc\")\n",
    "        prettify_node(node, label_color=\"black\", circle_color=circle_color, circle_size=8)\n",
    "\n",
    "    ts.layout_fn = layout_fn\n",
    "\n",
    "    outfile = os.path.join(outdir, output_image)\n",
    "    ete_tree.render(outfile, w=800, units=\"px\", tree_style=ts)\n",
    "    print(f\"Saved ETE tree ({'Circular' if layout == 'c' else 'Rectangular'}) to {outfile}\")\n",
    "\n",
    "##############################################################################\n",
    "# 4) Main function to produce the four chart types with ETE\n",
    "##############################################################################\n",
    "def build_ete_taxonomy_dendrograms(\n",
    "    elo_file=\"elo_results_with_metrics.json\",\n",
    "    feature_type=\"top_repetitive_words\",\n",
    "    top_n=1000,\n",
    "    outdir=\"phylo_trees\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads data, builds a presence/absence matrix for the given feature_type,\n",
    "    does hierarchical clustering (SciPy), converts to ETE tree, and saves two \n",
    "    ETE plots: circular and rectangular.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------\n",
    "    # 1) Load data & gather top N features\n",
    "    # -------------------------------------------------------------------\n",
    "    with open(elo_file, 'r', encoding='utf-8') as f:\n",
    "        elo_data = json.load(f)\n",
    "\n",
    "    # Collect features\n",
    "    model_to_features = {}\n",
    "    for model_name, model_info in elo_data.items():\n",
    "        # If this model isn't in the known family dict, we still handle it => \"Other\"\n",
    "        if feature_type == \"top_repetitive_words\":\n",
    "            feature_list = model_info.get(\"top_repetitive_words\", [])\n",
    "            top_features = []\n",
    "            if feature_list:\n",
    "                sorted_feats = sorted(feature_list, key=lambda x: x.get('score', 0), reverse=True)\n",
    "                top_features = [x[\"word\"] for x in sorted_feats[:top_n]]\n",
    "\n",
    "        elif feature_type == \"top_multi_prompt_bigrams\":\n",
    "            feature_list = model_info.get(\"top_multi_prompt_bigrams\", [])\n",
    "            top_features = []\n",
    "            if feature_list:\n",
    "                sorted_feats = sorted(feature_list, key=lambda x: x.get('frequency', 0), reverse=True)\n",
    "                top_features = [x[\"ngram\"] for x in sorted_feats[:top_n]]\n",
    "\n",
    "        elif feature_type == \"top_multi_prompt_trigrams\":\n",
    "            feature_list = model_info.get(\"top_multi_prompt_trigrams\", [])\n",
    "            top_features = []\n",
    "            if feature_list:\n",
    "                sorted_feats = sorted(feature_list, key=lambda x: x.get('frequency', 0), reverse=True)\n",
    "                top_features = [x[\"ngram\"] for x in sorted_feats[:top_n]]\n",
    "\n",
    "        elif feature_type == \"combined\":\n",
    "            # approximate each type gets top_n//3\n",
    "            words = model_info.get(\"top_repetitive_words\", [])\n",
    "            bigrams = model_info.get(\"top_multi_prompt_bigrams\", [])\n",
    "            trigrams = model_info.get(\"top_multi_prompt_trigrams\", [])\n",
    "            w_count = top_n // 3\n",
    "            b_count = top_n // 3\n",
    "            t_count = top_n // 3\n",
    "\n",
    "            top_words, top_bigrams, top_trigrams = [], [], []\n",
    "            if words:\n",
    "                sorted_w = sorted(words, key=lambda x: x.get('score', 0), reverse=True)\n",
    "                top_words = [x[\"word\"] for x in sorted_w[:w_count]]\n",
    "            if bigrams:\n",
    "                sorted_b = sorted(bigrams, key=lambda x: x.get('frequency', 0), reverse=True)\n",
    "                top_bigrams = [x[\"ngram\"] for x in sorted_b[:b_count]]\n",
    "            if trigrams:\n",
    "                sorted_t = sorted(trigrams, key=lambda x: x.get('frequency', 0), reverse=True)\n",
    "                top_trigrams = [x[\"ngram\"] for x in sorted_t[:t_count]]\n",
    "\n",
    "            top_features = top_words + top_bigrams + top_trigrams\n",
    "\n",
    "        else:\n",
    "            top_features = []  # unknown feature type\n",
    "\n",
    "        # If feature_type is n-gram based, convert lists to tuples\n",
    "        if feature_type in (\"top_multi_prompt_bigrams\", \"top_multi_prompt_trigrams\", \"combined\"):\n",
    "            top_features = [tuple(x) if isinstance(x, list) else x for x in top_features]\n",
    "        \n",
    "        # Convert n-gram lists to tuples (if they aren't already)\n",
    "        top_features = [tuple(x) if isinstance(x, list) else x for x in top_features]\n",
    "        model_to_features[model_name] = set(top_features)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 2) Build presence/absence DF\n",
    "    # -------------------------------------------------------------------\n",
    "    all_models = sorted(model_to_features.keys())  # all models found in JSON\n",
    "    from collections import Counter\n",
    "    duplicates = [model for model, count in Counter(all_models).items() if count > 1]\n",
    "    if duplicates:\n",
    "        print(\"Duplicate model names found:\", duplicates)\n",
    "    all_models = sorted(set(all_models))\n",
    "    \n",
    "    # Build global vocab\n",
    "    global_vocab = set()\n",
    "    for feats in model_to_features.values():\n",
    "        global_vocab.update(feats)\n",
    "\n",
    "    cleaned_vocab = []\n",
    "    for feat in global_vocab:\n",
    "        if isinstance(feat, str):\n",
    "            cleaned_vocab.append( (feat,) )\n",
    "        else:\n",
    "            cleaned_vocab.append(feat)\n",
    "\n",
    "    global_vocab = sorted(cleaned_vocab)\n",
    "    # global_vocab = sorted(list(global_vocab))\n",
    "\n",
    "    if not global_vocab or len(all_models) < 2:\n",
    "        print(f\"Skipping {feature_type} => Not enough data or not enough models.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(0, index=all_models, columns=global_vocab, dtype=int)\n",
    "    for m in all_models:\n",
    "        features_of_m = model_to_features[m]\n",
    "        for ft in features_of_m:\n",
    "            if ft not in df.columns:\n",
    "                # This can happen if feature is not in global_vocab for some reason\n",
    "                continue\n",
    "            try:\n",
    "                # direct integer positions\n",
    "                i = df.index.get_loc(m)\n",
    "                j = df.columns.get_loc(ft)\n",
    "                df.iloc[i, j] = 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error assigning df.loc[{m}, {ft}]\")\n",
    "                print(\"Feature type:\", type(ft), \"Value:\", ft)\n",
    "                raise\n",
    "\n",
    "    if len(df) < 2:\n",
    "        print(f\"Skipping {feature_type} => fewer than 2 models found.\")\n",
    "        return\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 3) SciPy clustering\n",
    "    # -------------------------------------------------------------------\n",
    "    dist_mat = pdist(df.values, metric='jaccard')\n",
    "    linked = linkage(dist_mat, method='complete')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 4) Convert to ETE Tree\n",
    "    # -------------------------------------------------------------------\n",
    "    ete_tree = linkage_to_ete_tree(linked, df.index.tolist())\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 5) Render circular and rectangular ETE dendrograms\n",
    "    # -------------------------------------------------------------------\n",
    "    circular_png = f\"ete_circular_{feature_type}.png\"\n",
    "    rectangular_png = f\"ete_rectangular_{feature_type}.png\"\n",
    "\n",
    "    render_ete_tree(\n",
    "        ete_tree,\n",
    "        output_image=circular_png,\n",
    "        layout=\"c\",\n",
    "        model_to_family=model_to_family,\n",
    "        family_colors=family_colors, \n",
    "        outdir=outdir,\n",
    "    )\n",
    "    render_ete_tree(\n",
    "        ete_tree,\n",
    "        output_image=rectangular_png,\n",
    "        layout=\"r\",\n",
    "        model_to_family=model_to_family,\n",
    "        family_colors=family_colors, \n",
    "        outdir=outdir,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming you've already got your DataFrame `df` with columns:\n",
    "#   [emobench_id, response_string, llm_responder, ...]\n",
    "# and the library functions are either in scope or imported.\n",
    "\n",
    "def build_elo_like_json(df, top_k=200, top_n_words=1000, min_prompt_ids=2):\n",
    "    \"\"\"\n",
    "    For each LLM in df, gather the top bigrams, trigrams, and repetitive words.\n",
    "    Returns a dict structure ready to be saved as JSON.\n",
    "    \"\"\"\n",
    "    # This dictionary will hold all data for the eventual JSON\n",
    "    elo_data = {}\n",
    "\n",
    "    # Group the DataFrame by llm_responder, so we process each model separately\n",
    "    grouped = df.groupby(\"llm_responder\")\n",
    "\n",
    "    for llm_name, group_df in grouped:\n",
    "        # 1) Build prompts_data = { prompt_id: [list of responses], ... }\n",
    "        prompts_data = defaultdict(list)\n",
    "        # 2) Build texts_with_ids_list = [(response_string, prompt_id), ...]\n",
    "        texts_with_ids_list = []\n",
    "\n",
    "        for row in group_df.itertuples(index=False):\n",
    "            text = row.response_string\n",
    "            prompt_id = str(row.emobench_id)  # cast to str or keep as int\n",
    "            # Add this text to the dictionary of prompts_data\n",
    "            prompts_data[prompt_id].append(text)\n",
    "            # Also keep track of (text, prompt_id) pairs\n",
    "            texts_with_ids_list.append((text, prompt_id))\n",
    "\n",
    "        # Initialize this model's entry\n",
    "        elo_data[llm_name] = {}\n",
    "\n",
    "        # 3) Calculate top bigrams and trigrams\n",
    "        try:\n",
    "            top_bigrams = get_multi_prompt_ngrams(prompts_data, n=2, top_k=top_k, min_prompt_ids=min_prompt_ids)\n",
    "            elo_data[llm_name][\"top_multi_prompt_bigrams\"] = [\n",
    "                {\"ngram\": list(ngram), \"frequency\": freq} for (ngram, freq) in top_bigrams\n",
    "            ]\n",
    "        except NameError:\n",
    "            print(\"Function `get_multi_prompt_ngrams` not found, skipping bigrams.\")\n",
    "            elo_data[llm_name][\"top_multi_prompt_bigrams\"] = []\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating bigrams for {llm_name}: {e}\")\n",
    "            elo_data[llm_name][\"top_multi_prompt_bigrams\"] = []\n",
    "\n",
    "        try:\n",
    "            top_trigrams = get_multi_prompt_ngrams(prompts_data, n=3, top_k=top_k, min_prompt_ids=min_prompt_ids)\n",
    "            elo_data[llm_name][\"top_multi_prompt_trigrams\"] = [\n",
    "                {\"ngram\": list(ngram), \"frequency\": freq} for (ngram, freq) in top_trigrams\n",
    "            ]\n",
    "        except NameError:\n",
    "            print(\"Function `get_multi_prompt_ngrams` not found, skipping trigrams.\")\n",
    "            elo_data[llm_name][\"top_multi_prompt_trigrams\"] = []\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating trigrams for {llm_name}: {e}\")\n",
    "            elo_data[llm_name][\"top_multi_prompt_trigrams\"] = []\n",
    "\n",
    "        # 4) Calculate top repetitive words\n",
    "        try:\n",
    "            top_repetitive_words = get_top_repetitive_words(\n",
    "                texts_with_ids_list, top_n=top_n_words, min_prompt_ids=min_prompt_ids\n",
    "            )\n",
    "            elo_data[llm_name][\"top_repetitive_words\"] = [\n",
    "                {\"word\": w, \"score\": score} for (w, score) in top_repetitive_words\n",
    "            ]\n",
    "        except NameError:\n",
    "            print(\"Function `get_top_repetitive_words` not found, skipping repetitive words.\")\n",
    "            elo_data[llm_name][\"top_repetitive_words\"] = []\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting repetitive words for {llm_name}: {e}\")\n",
    "            elo_data[llm_name][\"top_repetitive_words\"] = []\n",
    "\n",
    "        # You can also call your repetition metric function, etc. if desired.\n",
    "        # e.g.: repetition_score = calculate_repetition_metric(texts_with_ids_list, ...)\n",
    "        # elo_data[llm_name][\"repetition_score\"] = repetition_score\n",
    "\n",
    "    return elo_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an output directory for the JSON file.\n",
    "outdir = \"phylo_trees\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "      Extracting 2-grams... Done (200 texts).\n",
      "      Extracting 3-grams... Done (200 texts).\n",
      "Wrote results to phylo_trees/ngram_metrics.json.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1) Load your dataset\n",
    "dataset = load_dataset(\"llm-council/emotional_application\", \"response_collection\")\n",
    "df = dataset[\"council\"].to_pandas()\n",
    "\n",
    "# 2) Build the JSON-like dictionary\n",
    "elo_data = build_elo_like_json(df)\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)  # Ensure output directory exists)\n",
    "\n",
    "ngram_metrics_file = os.path.join(outdir, \"ngram_metrics.json\")\n",
    "\n",
    "# 3) Write it to file\n",
    "with open(ngram_metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(elo_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Wrote results to {ngram_metrics_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Building ETE Dendrograms for top_repetitive_words (top_n=1000) ----\n",
      "Saved ETE tree (Circular) to phylo_trees/ete_circular_top_repetitive_words.png\n",
      "Saved ETE tree (Rectangular) to phylo_trees/ete_rectangular_top_repetitive_words.png\n",
      "\n",
      "---- Building ETE Dendrograms for top_multi_prompt_bigrams (top_n=1000) ----\n",
      "Saved ETE tree (Circular) to phylo_trees/ete_circular_top_multi_prompt_bigrams.png\n",
      "Saved ETE tree (Rectangular) to phylo_trees/ete_rectangular_top_multi_prompt_bigrams.png\n",
      "\n",
      "---- Building ETE Dendrograms for top_multi_prompt_trigrams (top_n=1000) ----\n",
      "Saved ETE tree (Circular) to phylo_trees/ete_circular_top_multi_prompt_trigrams.png\n",
      "Saved ETE tree (Rectangular) to phylo_trees/ete_rectangular_top_multi_prompt_trigrams.png\n",
      "\n",
      "---- Building ETE Dendrograms for combined (top_n=1500) ----\n",
      "Saved ETE tree (Circular) to phylo_trees/ete_circular_combined.png\n",
      "Saved ETE tree (Rectangular) to phylo_trees/ete_rectangular_combined.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This tries to build 4 charts:\n",
    "#   1) top_repetitive_words\n",
    "#   2) top_multi_prompt_bigrams\n",
    "#   3) top_multi_prompt_trigrams\n",
    "#   4) combined\n",
    "chart_types = [\n",
    "    (\"top_repetitive_words\", 1000),\n",
    "    (\"top_multi_prompt_bigrams\", 1000),\n",
    "    (\"top_multi_prompt_trigrams\", 1000),\n",
    "    (\"combined\", 1500)\n",
    "]\n",
    "for feat_type, top_n in chart_types:\n",
    "    print(f\"---- Building ETE Dendrograms for {feat_type} (top_n={top_n}) ----\")\n",
    "    build_ete_taxonomy_dendrograms(\n",
    "        elo_file=ngram_metrics_file,\n",
    "        feature_type=feat_type,\n",
    "        top_n=top_n,\n",
    "        outdir=outdir,\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
