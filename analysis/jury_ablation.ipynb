{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92201dc-3da7-4426-929c-5105623b5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "REFERENCE_LLM_JUDGE = \"qwen1.5-32B-Chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fd383-e5cf-46e9-a8be-2683bb258e45",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95858d-c235-4f63-8783-7bf4d495201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"llm-council/emotional_application\", \"response_judging\")\n",
    "df = dataset[\"council\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979edba-e473-4291-8e83-69d9f8cda46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 10% of the data for faster simulations.\n",
    "df = df[df[\"emobench_id\"].isin(range(100, 110))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dddda5-2c58-4f6e-9a87-5ec61769ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa1633-c6d2-48d2-853a-35e8956dd77b",
   "metadata": {},
   "source": [
    "# Library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba941b3-2c95-4e60-b0a9-cf5a5cd03741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "council_members = df[\"llm_judge\"].unique()\n",
    "\n",
    "\n",
    "def calculate_non_overlapping_percentage(\n",
    "    df, model_column_name=\"model\", lower_column_name=\"lower\", upper_column_name=\"upper\"\n",
    "):\n",
    "    \"\"\"Returns the percentage of non-overlapping intervals.\"\"\"\n",
    "    # Extract the model names and intervals\n",
    "    models = df[model_column_name]\n",
    "    intervals = df[[lower_column_name, upper_column_name]]\n",
    "\n",
    "    # Calculate all possible pairs of models\n",
    "    total_pairs = 0\n",
    "    non_overlapping_pairs = 0\n",
    "\n",
    "    for (model1, interval1), (model2, interval2) in combinations(\n",
    "        zip(models, intervals.itertuples(index=False, name=None)), 2\n",
    "    ):\n",
    "        total_pairs += 1\n",
    "        lower1, upper1 = interval1\n",
    "        lower2, upper2 = interval2\n",
    "\n",
    "        # Check if intervals do not overlap\n",
    "        if upper1 < lower2 or upper2 < lower1:\n",
    "            non_overlapping_pairs += 1\n",
    "\n",
    "    # Calculate the percentage of non-overlapping pairs\n",
    "    if total_pairs > 0:\n",
    "        percentage = (non_overlapping_pairs / total_pairs) * 100\n",
    "    else:\n",
    "        percentage = 0  # In case there are not enough models to form a pair\n",
    "\n",
    "    return percentage\n",
    "\n",
    "\n",
    "\n",
    "def get_win_rate_column(df, column, baseline):\n",
    "    \"\"\"Uses Terry-Bradley to get the predicted win rate from elo stats for a specific model.\"\"\"\n",
    "    to_dict = df[[\"model\", column]].set_index(\"model\").to_dict()[column]\n",
    "    win_rate_table = predict_win_rate(to_dict)\n",
    "    return win_rate_table[baseline].fillna(0.5).apply(lambda x: round(x * 100, 2))\n",
    "\n",
    "\n",
    "def get_win_rate(bootstrap_online_elo, reference_llm_completer):\n",
    "    \"\"\"Uses Terry-Bradley to get the predicted win rate from elo stats for all models.\"\"\"\n",
    "    stats = pd.DataFrame()\n",
    "    stats[\"results\"] = None\n",
    "    stats[\"results\"] = stats[\"results\"].astype(\"object\")\n",
    "\n",
    "    for i, model in enumerate(bootstrap_online_elo.index):\n",
    "        stats.at[i, \"model\"] = model\n",
    "        stats.at[i, \"score\"] = bootstrap_online_elo[model]\n",
    "\n",
    "    return get_win_rate_column(stats, \"score\", reference_llm_completer)\n",
    "\n",
    "\n",
    "def predict_win_rate(elo_ratings, SCALE=400, BASE=10, INIT_RATING=1000):\n",
    "    \"\"\"Predicts the win rate given ELO ratings, a specified scale, base, and init rating.\"\"\"\n",
    "    names = sorted(list(elo_ratings.keys()))\n",
    "    wins = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for a in names:\n",
    "        for b in names:\n",
    "            ea = 1 / (1 + BASE ** ((elo_ratings[b] - elo_ratings[a]) / SCALE))\n",
    "            wins[a][b] = ea\n",
    "            wins[b][a] = 1 - ea\n",
    "\n",
    "    data = {a: [wins[a][b] if a != b else np.NAN for b in names] for a in names}\n",
    "\n",
    "    df = pd.DataFrame(data, index=names)\n",
    "    df.index.name = \"model_a\"\n",
    "    df.columns.name = \"model_b\"\n",
    "    return df.T\n",
    "\n",
    "def get_battles_from_judgment(df, WEIGHT=3):\n",
    "    \"\"\"Maps battle outcomes to specific winners. The WEIGHT is used to weight strong votes.\"\"\"\n",
    "    # Modified from https://github.com/lm-sys/arena-hard-auto/blob/main/show_result.py#L112C1-L176C30\n",
    "    battles = []\n",
    "    for _, row in df.iterrows():\n",
    "        output = {\n",
    "            \"question_id\": row[\"emobench_id\"],\n",
    "            \"model_a\": row[\"first_completion_by\"],\n",
    "            \"model_b\": row[\"second_completion_by\"],\n",
    "        }\n",
    "\n",
    "        weight = 1\n",
    "        if row[\"pairwise_choice\"] == \"A=B\":\n",
    "            output[\"winner\"] = \"tie\"\n",
    "        elif row[\"pairwise_choice\"] == \"A>B\":\n",
    "            output[\"winner\"] = \"model_a\"\n",
    "        elif row[\"pairwise_choice\"] == \"A>>B\":\n",
    "            output[\"winner\"] = \"model_a\"\n",
    "            weight = WEIGHT\n",
    "        elif row[\"pairwise_choice\"] == \"B>A\":\n",
    "            output[\"winner\"] = \"model_b\"\n",
    "        elif row[\"pairwise_choice\"] == \"B>>A\":\n",
    "            output[\"winner\"] = \"model_b\"\n",
    "            weight = WEIGHT\n",
    "        else:\n",
    "            print(\"Unknown pairwise_choice: \" + row[\"pairwise_choice\"])\n",
    "\n",
    "        for i in range(weight):\n",
    "            battles.append(output)\n",
    "\n",
    "    return pd.DataFrame(battles)\n",
    "\n",
    "\n",
    "def compute_mle_elo(df, reference_llm_completer, SCALE=400, BASE=10, INIT_RATING=1000):\n",
    "    \"\"\"Compute the ELO scores for all models.\"\"\"\n",
    "    df = get_battles_from_judgment(df)\n",
    "\n",
    "    models = pd.concat([df[\"model_a\"], df[\"model_b\"]]).unique()\n",
    "    models = pd.Series(np.arange(len(models)), index=models)\n",
    "\n",
    "    # duplicate battles\n",
    "    df = pd.concat([df, df], ignore_index=True)\n",
    "    p = len(models.index)\n",
    "    n = df.shape[0]\n",
    "\n",
    "    X = np.zeros([n, p])\n",
    "    X[np.arange(n), models[df[\"model_a\"]]] = +math.log(BASE)\n",
    "    X[np.arange(n), models[df[\"model_b\"]]] = -math.log(BASE)\n",
    "\n",
    "    # one A win => two A win\n",
    "    Y = np.zeros(n)\n",
    "    Y[df[\"winner\"] == \"model_a\"] = 1.0\n",
    "\n",
    "    # one tie => one A win + one B win\n",
    "    # Counts 1 tie as 1 A win and 1 B win, which is why we duplicate the battles.\n",
    "    tie_idx = df[\"winner\"] == \"tie\"\n",
    "    tie_idx[len(tie_idx) // 2 :] = False\n",
    "    Y[tie_idx] = 1.0\n",
    "\n",
    "    lr = LogisticRegression(fit_intercept=False, penalty=None, tol=1e-8)\n",
    "    lr.fit(X, Y)\n",
    "\n",
    "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
    "\n",
    "    # set anchor as reference_llm_completer = 1000\n",
    "    if reference_llm_completer in models.index:\n",
    "        elo_scores += 1000 - elo_scores[models[reference_llm_completer]]\n",
    "    return pd.Series(elo_scores, index=models.index).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def sample_combinations(items, choose, instances):\n",
    "    \"\"\"Generate all possible combinations of the specified size, and sample them randomly.\"\"\"\n",
    "    all_combinations = list(itertools.combinations(items, choose))\n",
    "    return random.choices(all_combinations, k=instances)\n",
    "\n",
    "\n",
    "# def filter_ratings_with_self_grading(df):\n",
    "#     # Without self-grading.\n",
    "#     df_filtered = df[df[\"llm_judge\"] != df[\"first_completion_by\"]]\n",
    "#     df_filtered = df_filtered[\n",
    "#         df_filtered[\"llm_judge\"] != df_filtered[\"second_completion_by\"]\n",
    "#     ]\n",
    "#     return df_filtered\n",
    "\n",
    "\n",
    "def filter_ratings_by_allowlist(df, llm_judge_allowlist):\n",
    "    \"\"\"Filter ratings by a list of allowed judges.\"\"\"\n",
    "    return df[df[\"llm_judge\"].isin(llm_judge_allowlist)]\n",
    "\n",
    "\n",
    "def get_adversarial_votes(df, num_adversarial_judges):\n",
    "    \"\"\"Get a dataframe of adversarial judge ratings (totally random).\"\"\"\n",
    "    all_dfs = [\n",
    "        get_adversarial_judge_ratings(df) for i in range(num_adversarial_judges)\n",
    "    ]\n",
    "    return pd.concat(all_dfs)\n",
    "\n",
    "\n",
    "def get_adversarial_judge_ratings(df):\n",
    "    \"\"\"Get ratings for adversarial judges.\"\"\"\n",
    "    # Extract unique combinations of 'id', 'first_completion_by', 'second_completion_by'\n",
    "    adversarial_judge_df = df[\n",
    "        [\"emobench_id\", \"first_completion_by\", \"second_completion_by\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # Assign fixed values\n",
    "    adversarial_judge_df[\"llm_judge\"] = \"adversarial\"\n",
    "    adversarial_judge_df[\"metadata\"] = None\n",
    "\n",
    "    # Assign random pairwise choices.\n",
    "    adversarial_judge_df[\"pairwise_choice\"] = np.random.choice(\n",
    "        [\"A>B\", \"B>A\", \"A>>B\", \"B>>A\"], size=len(adversarial_judge_df)\n",
    "    )\n",
    "    return adversarial_judge_df\n",
    "\n",
    "\n",
    "def get_simulated_leaderboard_with_adversarial_judges(\n",
    "    df,\n",
    "    council_members,\n",
    "    jury_size,\n",
    "    num_samples,\n",
    "    num_adversarial_judges,\n",
    "    num_adversarial_judges_frac,\n",
    "    adversarial_method,\n",
    "    simulation_normalized_number_judges,\n",
    "    reference_llm_completer,\n",
    "):\n",
    "    \"\"\"Get a simulated leaderboard with adversarial judges.\"\"\"\n",
    "    win_rate_dict = defaultdict(list)\n",
    "    rank_dict = defaultdict(list)\n",
    "    elo_dict = defaultdict(list)\n",
    "    sampled_jury_compositions = sample_combinations(\n",
    "        council_members, jury_size, num_samples\n",
    "    )\n",
    "\n",
    "    council_win_rate_dict = defaultdict(list)\n",
    "    council_rank_dict = defaultdict(list)\n",
    "    council_elo_dict = defaultdict(list)\n",
    "\n",
    "    # Determine the number of adversarial judges.\n",
    "    num_real_judges = len(sampled_jury_compositions[0])\n",
    "    if num_adversarial_judges_frac:\n",
    "        # 1, 0.5 -> 2, 5, 0.5 -> 7\n",
    "        num_judges_including_adversaries = int(\n",
    "            num_real_judges / num_adversarial_judges_frac\n",
    "        )\n",
    "        num_adversarial_judges = num_judges_including_adversaries - num_real_judges\n",
    "    elif num_adversarial_judges:\n",
    "        num_judges_including_adversaries = num_real_judges + num_adversarial_judges\n",
    "    else:\n",
    "        num_judges_including_adversaries = num_real_judges\n",
    "        num_adversarial_judges = 0\n",
    "    print(f\"- Number of judges: {num_real_judges}\")\n",
    "    print(f\"- Number of adversarial judges: {num_adversarial_judges}\")\n",
    "    print(\n",
    "        f\"- Number of judges including adversaries: {num_judges_including_adversaries}\"\n",
    "    )\n",
    "\n",
    "    for sampled_jury_composition in sampled_jury_compositions:\n",
    "        filtered_df = filter_ratings_by_allowlist(df, sampled_jury_composition)\n",
    "\n",
    "        # Add adversarial judges, if applicable.\n",
    "        if num_adversarial_judges:\n",
    "            adversarial_df = get_adversarial_votes(\n",
    "                filtered_df, num_adversarial_judges, adversarial_method\n",
    "            )\n",
    "            filtered_df = pd.concat([adversarial_df, filtered_df])\n",
    "\n",
    "        # Basically council majority aggregation.\n",
    "        filtered_df = (\n",
    "            filtered_df.groupby([\"emobench_id\", \"first_completion_by\", \"second_completion_by\"])[\n",
    "                \"pairwise_choice\"\n",
    "            ]\n",
    "            .agg(lambda x: x.mode()[0])\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        bootstrap_online_elo = compute_mle_elo(filtered_df, reference_llm_completer)\n",
    "\n",
    "        for llm_completer, elo_score in bootstrap_online_elo.to_dict().items():\n",
    "            elo_dict[llm_completer].append(elo_score)\n",
    "\n",
    "        win_rates = get_win_rate(bootstrap_online_elo, reference_llm_completer)\n",
    "        for llm_completer, win_rate in win_rates.to_dict().items():\n",
    "            win_rate_dict[llm_completer].append(win_rate)\n",
    "\n",
    "        ranks = bootstrap_online_elo.rank(method=\"min\", ascending=False)\n",
    "        for llm_completer, rank in ranks.items():\n",
    "            rank_dict[llm_completer].append(rank)\n",
    "\n",
    "    stats = pd.DataFrame()\n",
    "    for i, model in enumerate(bootstrap_online_elo.index):\n",
    "        stats.at[i, \"model\"] = model\n",
    "        stats.at[i, \"rank_score\"] = np.mean(rank_dict[model])\n",
    "        stats.at[i, \"rank_std\"] = np.std(rank_dict[model])\n",
    "        stats.at[i, \"rank_lower\"] = np.percentile(rank_dict[model], 2.5)\n",
    "        stats.at[i, \"rank_upper\"] = np.percentile(rank_dict[model], 97.5)\n",
    "\n",
    "        stats.at[i, \"elo_score\"] = np.mean(elo_dict[model])\n",
    "        stats.at[i, \"elo_std\"] = np.std(elo_dict[model])\n",
    "        stats.at[i, \"elo_lower\"] = np.percentile(elo_dict[model], 2.5)\n",
    "        stats.at[i, \"elo_upper\"] = np.percentile(elo_dict[model], 97.5)\n",
    "\n",
    "        stats.at[i, \"win_rate_score\"] = np.mean(win_rate_dict[model])\n",
    "        stats.at[i, \"win_rate_std\"] = np.std(win_rate_dict[model])\n",
    "        stats.at[i, \"win_rate_lower\"] = np.percentile(win_rate_dict[model], 2.5)\n",
    "        stats.at[i, \"win_rate_upper\"] = np.percentile(win_rate_dict[model], 97.5)\n",
    "\n",
    "    stats[\"separability\"] = calculate_non_overlapping_percentage(\n",
    "        stats,\n",
    "        model_column_name=\"model\",\n",
    "        lower_column_name=\"win_rate_lower\",\n",
    "        upper_column_name=\"win_rate_upper\",\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_jury_ablation_stats(\n",
    "    df,\n",
    "    num_samples,\n",
    "    council_members,\n",
    "    num_adversarial_judges,\n",
    "    num_adversarial_judges_frac,\n",
    "    adversarial_method,\n",
    "    reference_llm_completer,\n",
    "):\n",
    "    \"\"\"Get a simulated leaderboard with number of adversarial judges.\"\"\"\n",
    "    jury_ablation_stats_with_adversarial_judges = pd.DataFrame()\n",
    "\n",
    "    # Map None to 0.\n",
    "    if num_adversarial_judges is None:\n",
    "        num_adversarial_judges = 0\n",
    "    if num_adversarial_judges_frac is None:\n",
    "        num_adversarial_judges_frac = 0\n",
    "\n",
    "    if num_adversarial_judges:\n",
    "        simulation_normalized_number_judges = (\n",
    "            len(council_members) + num_adversarial_judges\n",
    "        )\n",
    "    elif num_adversarial_judges_frac:\n",
    "        simulation_normalized_number_judges = int(\n",
    "            len(council_members) / num_adversarial_judges_frac\n",
    "        )\n",
    "    else:\n",
    "        simulation_normalized_number_judges = len(council_members)\n",
    "    print(\n",
    "        f\"Number of situation-normalized council members: {simulation_normalized_number_judges}\"\n",
    "    )\n",
    "\n",
    "    for jury_size in range(1, len(council_members) + 1):\n",
    "        print(f\"Evaluating jury size: {jury_size}\")\n",
    "        num_retries_left = 3\n",
    "        while num_retries_left > 0:\n",
    "            try:\n",
    "                stats = get_simulated_leaderboard_with_adversarial_judges(\n",
    "                    df,\n",
    "                    council_members,\n",
    "                    jury_size,\n",
    "                    num_samples,\n",
    "                    num_adversarial_judges,\n",
    "                    num_adversarial_judges_frac,\n",
    "                    adversarial_method,\n",
    "                    simulation_normalized_number_judges,\n",
    "                    reference_llm_completer,\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Encountered separability issue: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                num_retries_left -= 1\n",
    "            \n",
    "        jury_ablation_stats_with_adversarial_judges.at[jury_size, \"rank_std.mean\"] = (\n",
    "            stats[\"rank_std\"].mean()\n",
    "        )\n",
    "        print(stats[\"rank_std\"].mean())\n",
    "        jury_ablation_stats_with_adversarial_judges.at[jury_size, \"rank_std.std\"] = (\n",
    "            stats[\"rank_std\"].std()\n",
    "        )\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"rank_std.ci.upper\"\n",
    "        ] = np.percentile(stats[\"rank_std\"], 97.5)\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"rank_std.ci.lower\"\n",
    "        ] = np.percentile(stats[\"rank_std\"], 2.5)\n",
    "        jury_ablation_stats_with_adversarial_judges.at[jury_size, \"elo_std.mean\"] = (\n",
    "            stats[\"elo_std\"].mean()\n",
    "        )\n",
    "        jury_ablation_stats_with_adversarial_judges.at[jury_size, \"elo_std.std\"] = (\n",
    "            stats[\"elo_std\"].std()\n",
    "        )\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"elo_std.ci.upper\"\n",
    "        ] = np.percentile(stats[\"elo_std\"], 97.5)\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"elo_std.ci.lower\"\n",
    "        ] = np.percentile(stats[\"elo_std\"], 2.5)\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"win_rate_std.mean\"\n",
    "        ] = stats[\"win_rate_std\"].mean()\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"win_rate_std.std\"\n",
    "        ] = stats[\"win_rate_std\"].std()\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"win_rate_std.ci.upper\"\n",
    "        ] = np.percentile(stats[\"win_rate_std\"], 97.5)\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"win_rate_std.ci.lower\"\n",
    "        ] = np.percentile(stats[\"win_rate_std\"], 2.5)\n",
    "\n",
    "        # Separability\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"separability.mean\"\n",
    "        ] = stats[\"separability\"].mean()\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"separability.std\"\n",
    "        ] = stats[\"separability\"].std()\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"separability.ci.upper\"\n",
    "        ] = np.percentile(stats[\"separability\"], 97.5)\n",
    "        jury_ablation_stats_with_adversarial_judges.at[\n",
    "            jury_size, \"separability.ci.lower\"\n",
    "        ] = np.percentile(stats[\"separability\"], 2.5)\n",
    "\n",
    "    print(\"Finished jury ablation.\")\n",
    "    return jury_ablation_stats_with_adversarial_judges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0226b8f-d972-44d9-9ac1-e0b20bfbbdf8",
   "metadata": {},
   "source": [
    "# Run simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f1d95-4cfc-4865-a023-942ac489cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without adversarial judges.\n",
    "num_samples = 100\n",
    "\n",
    "jury_ablation_stats = get_jury_ablation_stats(\n",
    "    df=df,\n",
    "    council_members=council_members,\n",
    "    num_samples=num_samples,\n",
    "    num_adversarial_judges=None,\n",
    "    num_adversarial_judges_frac=None,\n",
    "    adversarial_method=None,\n",
    "    reference_llm_completer=REFERENCE_LLM_JUDGE,\n",
    ")\n",
    "\n",
    "# With a fixed number of adversarial judges.\n",
    "jury_ablation_stats_with_adversarial_judges = get_jury_ablation_stats(\n",
    "    df=df,\n",
    "    council_members=council_members,\n",
    "    num_samples=num_samples,\n",
    "    num_adversarial_judges=10,\n",
    "    num_adversarial_judges_frac=None,\n",
    "    adversarial_method=\"random\",\n",
    "    reference_llm_completer=REFERENCE_LLM_JUDGE,\n",
    ")\n",
    "\n",
    "# A fixed percentage of adversarial members.\n",
    "jury_ablation_stats_with_adversarial_judges_fixed_percentage = (\n",
    "    get_jury_ablation_stats(\n",
    "        df=df,\n",
    "        council_members=council_members,\n",
    "        num_samples=num_samples,\n",
    "        num_adversarial_judges=None,\n",
    "        num_adversarial_judges_frac=0.5,\n",
    "        adversarial_method=\"random\",\n",
    "        reference_llm_completer=REFERENCE_LLM_JUDGE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac423ba-49db-42c1-bf99-ef6697161f80",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f42fec-feca-469e-83ac-5dc032783c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_combined_jury_ablation(jury_ablation_stats_map, main_stat_name, main_stat, main_stat_upper, main_stat_lower, title1, title2):\n",
    "    # Create a new figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Collect all unique jury sizes for setting x-ticks\n",
    "    all_jury_sizes = []\n",
    "\n",
    "    # Plot the first subplot (Variance of assigned rankings)\n",
    "    for (label, color), jury_ablation_stats in jury_ablation_stats_map.items():\n",
    "        # Append the jury sizes to the list\n",
    "        all_jury_sizes.extend(jury_ablation_stats.index.astype(int).tolist())\n",
    "        \n",
    "        axes[0].errorbar(\n",
    "            jury_ablation_stats.index.astype(int),\n",
    "            jury_ablation_stats[main_stat],\n",
    "            yerr=[\n",
    "                jury_ablation_stats[main_stat] - jury_ablation_stats[main_stat_lower],\n",
    "                jury_ablation_stats[main_stat_upper] - jury_ablation_stats[main_stat],\n",
    "            ],\n",
    "            fmt='o',  # 'o' for circular markers\n",
    "            label=label,\n",
    "            color=color,\n",
    "            capsize=3,  # Add caps to the error bars\n",
    "        )\n",
    "\n",
    "    xticks = sorted(set(all_jury_sizes))\n",
    "    # Select every 5th tick\n",
    "    xticks = [tick for i, tick in enumerate(xticks) if i % 5 == 0 or i == len(xticks) - 1]\n",
    "    axes[0].set_xticks(xticks)\n",
    "    axes[0].set_xticklabels(xticks, fontsize=12)\n",
    "    axes[0].set_xlabel(\"# Non-Adversarial Jury Members\", fontsize=12)\n",
    "    axes[0].set_title(\"Mean Variance of Rank (MVR)\", fontsize=12)\n",
    "    axes[0].tick_params(axis='both', which='major', labelsize=12)\n",
    "    axes[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "    # Plot the second subplot (Separability)\n",
    "    for (label, color), jury_ablation_stats in jury_ablation_stats_map.items():\n",
    "        df = pd.DataFrame({\n",
    "            'Jury Size': jury_ablation_stats.index.astype(int),  # Convert to numerical\n",
    "            'Separability': jury_ablation_stats['separability.mean']\n",
    "        })\n",
    "        \n",
    "        all_jury_sizes.extend(df['Jury Size'].unique())\n",
    "\n",
    "        sns.regplot(\n",
    "            ax=axes[1],\n",
    "            x='Jury Size', \n",
    "            y='Separability', \n",
    "            data=df, \n",
    "            label=label, \n",
    "            color=color, \n",
    "            scatter_kws={'s': 50},  # Size of points\n",
    "            line_kws={'linestyle': ':'}  # Dotted trendline\n",
    "        )\n",
    "\n",
    "    axes[1].set_xticks(xticks)\n",
    "    axes[1].set_xticklabels(xticks, fontsize=12)\n",
    "    axes[1].set_xlabel(\"# Non-Adversarial Jury Members\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"\", fontsize=12)\n",
    "    axes[1].tick_params(axis='both', which='major', labelsize=12)\n",
    "    axes[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "    axes[1].set_title(\"Separability\", fontsize=12)\n",
    "\n",
    "    # Create a single legend for both subplots\n",
    "    handles, labels = axes[1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(jury_ablation_stats_map), fontsize=12)\n",
    "\n",
    "    # Adjust layout to make room for the shared legend\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "    plt.savefig(\"/Users/justinzhao/Repos/llm-council/experiments/expansion.5_26/conversational_response_judging.100_to_200.qwen_32b_reference/jury_ablation_combined.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_combined_jury_ablation(\n",
    "    jury_ablation_stats_map={\n",
    "        (\"No adversaries\", \"tab:red\"): jury_ablation_stats,\n",
    "        (\"+10 adversaries\", \"tab:green\"): jury_ablation_stats_with_adversarial_judges,\n",
    "        (\"+50% adversaries\", \"tab:blue\"): jury_ablation_stats_with_adversarial_judges_fixed_percentage,\n",
    "    },\n",
    "    main_stat_name=\"rank σ\",\n",
    "    main_stat=\"rank_std.mean\", \n",
    "    main_stat_upper=\"rank_std.ci.upper\", \n",
    "    main_stat_lower=\"rank_std.ci.lower\",\n",
    "    title1=\"Variance of assigned LLM rankings over simulated jury compositions (n=100)\",\n",
    "    title2=\"Separability over simulated jury compositions (n=100)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d95ac1-de82-49d3-b904-1e5ad885e0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
