# Schema defined in llm_council/members/schema.py
- model_name: Llama-3.1-8B
  providers:
    - name: together
      fully_qualified_name: together://meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
      costs:
        serverless:
          per_1m_input_tokens: 0.2
          per_1m_output_tokens: 0.2
    - name: cerebras
      fully_qualified_name: cerebras://llama3.1-8b
      costs:
        serverless:
          per_1m_input_tokens: 0.1
          per_1m_output_tokens: 0.1
    - name: lepton
      fully_qualified_name: lepton://llama3-1-8b
      costs:
        serverless:
          per_1m_input_tokens: 0.07
          per_1m_output_tokens: 0.07
  model_info:
    context_window: 16384
    training_data_cutoff: December 2023
    release_date: July 23, 2024
    num_parameters: 8b
    modalities:
      - text

- model_name: Llama-3.1-70B
  providers:
    - name: together
      fully_qualified_name: together://meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
      costs:
        serverless:
          per_1m_input_tokens: 0.88
          per_1m_output_tokens: 0.88
    - name: cerebras
      fully_qualified_name: cerebras://llama3.1-70b
      costs:
        serverless:
          per_1m_input_tokens: 0.6
          per_1m_output_tokens: 0.6
    - name: lepton
      fully_qualified_name: lepton://llama3-1-70b
      costs:
        serverless:
          per_1m_input_tokens: 0.8
          per_1m_output_tokens: 0.8
  model_info:
    context_window: 131072
    training_data_cutoff: December 2023
    release_date: July 23, 2024
    num_parameters: 70b
    modalities:
      - text

- model_name: Llama-3.1-405B
  providers:
    - name: lepton
      fully_qualified_name: lepton://llama3-1-405b
      costs:
        serverless:
          per_1m_input_tokens: 2.8
          per_1m_output_tokens: 2.8
    - name: together
      fully_qualified_name: together://meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
      costs:
        serverless:
          per_1m_input_tokens: 3.5
          per_1m_output_tokens: 3.5
  model_info:
    context_window: 130815
    training_data_cutoff: December 2023
    release_date: July 23, 2024
    num_parameters: 405b
    modalities:
      - text