{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65743b35-1006-463e-8498-d80554e0a30a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24e90a-d058-415a-b7bc-b10c97be27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sam-paech/mmlu-pro-nomath-sml\")\n",
    "df_ground_truth = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048e543-b7ed-4e35-ad45-6049d6120bca",
   "metadata": {},
   "source": [
    "# Generate Metrics Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c79b7-86a6-4d1d-a597-37192e993b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from llm_council.utils import jsonl_io\n",
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "from llm_council.processors.council_service import (\n",
    "    CouncilService,\n",
    ")\n",
    "\n",
    "\n",
    "# Constants and helper functions remain the same\n",
    "CHOICE_MAP = {\n",
    "    0: \"A\",\n",
    "    1: \"B\",\n",
    "    2: \"C\",\n",
    "    3: \"D\",\n",
    "    4: \"E\",\n",
    "    5: \"F\",\n",
    "    6: \"G\",\n",
    "    7: \"H\",\n",
    "    8: \"I\",\n",
    "    9: \"J\",\n",
    "    10: \"K\",\n",
    "    11: \"L\",\n",
    "    12: \"M\",\n",
    "}\n",
    "\n",
    "def get_metadata_for_response_file(response_filename):\n",
    "    # Extract metadata from the response file path\n",
    "    parts = response_filename.split(os.sep)\n",
    "    # The structure is: data / mmlu_pro.n100.mini.runX / config / openai / model / responses.jsonl\n",
    "    run_dir = parts[1]  # e.g., 'mmlu_pro.n100.mini.run0'\n",
    "    config_dir = parts[2]  # e.g., 'pr_ans_cot1.no_schema.temp1.no_role'\n",
    "    llm = parts[4]  # e.g., 'gpt-4o-mini-2024-07-18'\n",
    "\n",
    "    # Extract run number\n",
    "    run_number = run_dir.split('.')[-1]  # e.g., 'run0'\n",
    "\n",
    "    # Extract metadata from the configuration directory\n",
    "    metadata_pieces = config_dir.split('.')\n",
    "    return {\n",
    "        \"prompt_template\": metadata_pieces[0],\n",
    "        \"schema_name\": metadata_pieces[1],\n",
    "        \"temperature\": metadata_pieces[2],\n",
    "        \"role\": metadata_pieces[3],\n",
    "        \"llm\": llm,\n",
    "        \"run_number\": run_number,\n",
    "    }\n",
    "\n",
    "def parse_my_assessment_substring(response_string):\n",
    "    response_string = response_string.lower()\n",
    "    if \"my assessment is\" in response_string:\n",
    "        response_string = (\n",
    "            response_string.replace(\"*\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        )\n",
    "        assessment = response_string.split(\"my assessment is \")[-1][0]\n",
    "        return assessment.upper()\n",
    "    if \"the answer is \" in response_string:\n",
    "        response_string = (\n",
    "            response_string.replace(\"*\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        )\n",
    "        assessment = response_string.split(\"the answer is \")[-1][0]\n",
    "        return assessment.upper()\n",
    "    return \"ERROR\"\n",
    "\n",
    "# New function to collect configurations and their response files across runs\n",
    "def collect_configurations(base_dir):\n",
    "    \"\"\"\n",
    "    Collects all configurations and their corresponding response files across runs.\n",
    "\n",
    "    Parameters:\n",
    "    - base_dir: The base directory containing all runs.\n",
    "\n",
    "    Returns:\n",
    "    - config_response_files: Dictionary mapping configurations to their response files across runs.\n",
    "    \"\"\"\n",
    "    # Use glob to find all response files\n",
    "    response_files = glob(os.path.join(base_dir, '**', 'responses.jsonl'), recursive=True)\n",
    "\n",
    "    # Map configurations to their response files\n",
    "    config_response_files = {}\n",
    "    for response_file in response_files:\n",
    "        metadata = get_metadata_for_response_file(response_file)\n",
    "        # Use a tuple of configuration parameters as the key\n",
    "        config_key = (\n",
    "            metadata['prompt_template'],\n",
    "            metadata['schema_name'],\n",
    "            metadata['temperature'],\n",
    "            metadata['role'],\n",
    "            metadata['llm']\n",
    "        )\n",
    "        if config_key not in config_response_files:\n",
    "            config_response_files[config_key] = []\n",
    "        config_response_files[config_key].append(response_file)\n",
    "    return config_response_files\n",
    "\n",
    "# Function to process a single configuration across runs\n",
    "def process_configuration(response_files, df_ground_truth, council_service, k=2):\n",
    "    \"\"\"\n",
    "    Processes a single configuration across multiple runs to compute metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - response_files: List of response files for this configuration across runs.\n",
    "    - df_ground_truth: DataFrame containing ground truth answers.\n",
    "    - k: Exponent for the Exponential Consistency Weighted Metric.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary of computed metrics for this configuration.\n",
    "    \"\"\"\n",
    "    total_runs = len(response_files)\n",
    "    per_run_accuracies = []\n",
    "    per_run_unparsable = []\n",
    "    per_run_uncertain = []\n",
    "    per_run_response_lengths = []\n",
    "    per_run_response_lengths_stdev = []\n",
    "\n",
    "    item_correct_counts = {}\n",
    "    item_ids = set()\n",
    "\n",
    "    df_all_runs = pd.DataFrame()\n",
    "\n",
    "    for run_idx, response_file in enumerate(response_files):\n",
    "        metadata = get_metadata_for_response_file(response_file)\n",
    "        prompt_template = metadata.get('prompt_template', '')\n",
    "        schema_name = metadata.get('schema_name', '')\n",
    "\n",
    "        num_items = 0\n",
    "        num_correct_items = 0\n",
    "        num_unparsable_items = 0\n",
    "        num_uncertain_items_for_judging_ground_truth = 0\n",
    "        response_lengths = []\n",
    "\n",
    "        item_ids_run = []\n",
    "        is_correct_list = []\n",
    "\n",
    "        print(f\"response_file is: {response_file}\")\n",
    "\n",
    "        with jsonlines.open(response_file) as reader:\n",
    "            for response_data in reader:\n",
    "                num_items += 1\n",
    "\n",
    "                response_string = council_service.get_llm_response_string(response_data)\n",
    "                response_lengths.append(len(response_string))\n",
    "\n",
    "                question_id = response_data[-1][\"completion_request\"][\"question_id\"]\n",
    "                item_ids_run.append(question_id)\n",
    "                item_ids.add(question_id)\n",
    "\n",
    "                df_row = df_ground_truth[df_ground_truth[\"question_id\"] == question_id]\n",
    "\n",
    "                if schema_name != \"no_schema\":\n",
    "                    try:\n",
    "                        json_payload = json.loads(response_string)\n",
    "                        parsed_assessment = json_payload.get(\"answer\", \"ERROR\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        parsed_assessment = \"ERROR\"\n",
    "                else:\n",
    "                    parsed_assessment = parse_my_assessment_substring(response_string)\n",
    "\n",
    "                if parsed_assessment == \"ERROR\":\n",
    "                    num_unparsable_items += 1\n",
    "                    is_correct = False\n",
    "                else:\n",
    "                    if \"ans\" in prompt_template:\n",
    "                        ground_truth = df_row[\"answer\"].item()\n",
    "                        is_correct = (parsed_assessment == ground_truth)\n",
    "                    else:\n",
    "                        # For prompts without 'ans', consider 'A' as correct\n",
    "                        is_correct = (parsed_assessment == \"A\")\n",
    "                        if parsed_assessment == \"C\":\n",
    "                            num_uncertain_items_for_judging_ground_truth += 1\n",
    "\n",
    "                    if is_correct:\n",
    "                        num_correct_items += 1\n",
    "\n",
    "                is_correct_list.append(is_correct)\n",
    "\n",
    "                # Update per-item correctness counts\n",
    "                if question_id not in item_correct_counts:\n",
    "                    item_correct_counts[question_id] = 0\n",
    "                if is_correct:\n",
    "                    item_correct_counts[question_id] += 1\n",
    "\n",
    "        # Compute metrics for this run\n",
    "        accuracy = num_correct_items / num_items if num_items > 0 else 0\n",
    "        per_run_accuracies.append(accuracy)\n",
    "\n",
    "        pct_unparsable_items = num_unparsable_items / num_items if num_items > 0 else 0\n",
    "        per_run_unparsable.append(pct_unparsable_items)\n",
    "\n",
    "        pct_uncertain_items = num_uncertain_items_for_judging_ground_truth / num_items if num_items > 0 else 0\n",
    "        per_run_uncertain.append(pct_uncertain_items)\n",
    "\n",
    "        response_length_mean = np.mean(response_lengths) if response_lengths else 0\n",
    "        response_length_stdev = np.std(response_lengths) if response_lengths else 0\n",
    "        per_run_response_lengths.append(response_length_mean)\n",
    "        per_run_response_lengths_stdev.append(response_length_stdev)\n",
    "\n",
    "        # Create DataFrame for this run's per-item correctness\n",
    "        df_run = pd.DataFrame({\n",
    "            'question_id': item_ids_run,\n",
    "            f'is_correct_run{run_idx}': is_correct_list\n",
    "        })\n",
    "\n",
    "        if df_all_runs.empty:\n",
    "            df_all_runs = df_run\n",
    "        else:\n",
    "            df_all_runs = df_all_runs.merge(df_run, on='question_id', how='outer')\n",
    "\n",
    "    # Compute per-item correctness fraction across runs\n",
    "    correctness_columns = [col for col in df_all_runs.columns if col.startswith('is_correct_run')]\n",
    "    df_all_runs['correct_count'] = df_all_runs[correctness_columns].sum(axis=1)\n",
    "    df_all_runs['fraction_correct'] = df_all_runs['correct_count'] / total_runs\n",
    "    df_all_runs['weighted_score'] = df_all_runs['fraction_correct'] ** k\n",
    "\n",
    "    # Compute Exponential Consistency Weighted Metric\n",
    "    ecwm = df_all_runs['weighted_score'].mean()\n",
    "\n",
    "    # Compute overall metrics\n",
    "    accuracy_mean = np.mean(per_run_accuracies)\n",
    "    accuracy_stdev = np.std(per_run_accuracies)\n",
    "    unparsable_mean = np.mean(per_run_unparsable)\n",
    "    unparsable_stdev = np.std(per_run_unparsable)\n",
    "    uncertain_mean = np.mean(per_run_uncertain)\n",
    "    uncertain_stdev = np.std(per_run_uncertain)\n",
    "    response_length_mean = np.mean(per_run_response_lengths)\n",
    "    response_length_stdev = np.std(per_run_response_lengths)\n",
    "    response_length_stdev_mean = np.mean(per_run_response_lengths_stdev)\n",
    "    response_length_stdev_stdev = np.std(per_run_response_lengths_stdev)\n",
    "\n",
    "    # Prepare the metrics dictionary\n",
    "    metrics = {\n",
    "        'accuracy_mean': accuracy_mean,\n",
    "        'accuracy_stdev': accuracy_stdev,\n",
    "        f'exponential_consistency_weighted_metric_k{k}': ecwm,\n",
    "        'pct_unparsable_items_mean': unparsable_mean,\n",
    "        'pct_unparsable_items_stdev': unparsable_stdev,\n",
    "        'pct_uncertain_items_mean': uncertain_mean,\n",
    "        'pct_uncertain_items_stdev': uncertain_stdev,\n",
    "        'response_length_mean': response_length_mean,\n",
    "        'response_length_stdev': response_length_stdev,\n",
    "        'response_length_stdev_mean': response_length_stdev_mean,\n",
    "        'response_length_stdev_stdev': response_length_stdev_stdev,\n",
    "        'num_items': len(item_ids),\n",
    "        'num_runs': total_runs,\n",
    "    }\n",
    "\n",
    "    # Include configuration metadata\n",
    "    config_metadata = {\n",
    "        'prompt_template': metadata['prompt_template'],\n",
    "        'schema_name': metadata['schema_name'],\n",
    "        'temperature': metadata['temperature'],\n",
    "        'role': metadata['role'],\n",
    "        'llm': metadata['llm'],\n",
    "        'cot_type': metadata['prompt_template'].split('_')[-1],\n",
    "    }\n",
    "\n",
    "    metrics.update(config_metadata)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Main function to process all configurations\n",
    "def process_all_configurations(base_dir, df_ground_truth, council_service, k=2):\n",
    "    \"\"\"\n",
    "    Processes all configurations across runs and compiles metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - base_dir: Base directory containing all runs.\n",
    "    - df_ground_truth: DataFrame containing ground truth answers.\n",
    "    - k: Exponent for the Exponential Consistency Weighted Metric.\n",
    "\n",
    "    Returns:\n",
    "    - df_final_metrics: DataFrame containing metrics for all configurations.\n",
    "    \"\"\"\n",
    "    config_response_files = collect_configurations(base_dir)\n",
    "\n",
    "    all_metrics = []\n",
    "    for config_key, response_files in config_response_files.items():\n",
    "        # Process this configuration\n",
    "        metrics = process_configuration(response_files, df_ground_truth, council_service, k)\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_final_metrics = pd.DataFrame(all_metrics)\n",
    "\n",
    "    return df_final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42226196-82c0-47bb-8833-1a2c19aede55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics.\n",
    "# Base directory containing all runs\n",
    "base_dir = 'data_mmlu'\n",
    "\n",
    "# Initialize council service with the llms that you want to analyze.\n",
    "council_service = CouncilService(\n",
    "    llm_council_members=[\n",
    "        \"lepton://llama3-1-8b\",\n",
    "        \"lepton://llama3-1-70b\",\n",
    "        \"openai://gpt-4o-mini-2024-07-18\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process all configurations\n",
    "df_metrics = process_all_configurations(base_dir, df_ground_truth, council_service, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9523d53-b882-438d-a37b-021c3a41e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd35e5-b008-47bb-872c-5d1ec0b0b382",
   "metadata": {},
   "source": [
    "# Plotting Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8a8a8-6da2-4be1-b3f1-70cc21df8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# \"lepton://llama3-1-8b\",\n",
    "# \"lepton://llama3-1-70b\",\n",
    "# \"openai://gpt-4o-mini-2024-07-18\"\n",
    "LLM_TO_ANALYZE = \"llama3-1-70b\"\n",
    "\n",
    "\n",
    "# Function to classify schema names as \"Unconstrained\" or \"Structured Output\"\n",
    "def classify_schema(schema_name):\n",
    "    if schema_name.lower() == 'no_schema':\n",
    "        return 'Unconstrained'\n",
    "    else:\n",
    "        return 'Structured Output'\n",
    "\n",
    "# Mapping for renaming CoT types\n",
    "cot_labels = {\n",
    "    'cot0': 'No CoT',\n",
    "    'cot1': 'CoT then Answer',\n",
    "    'cot2': 'Answer then CoT'\n",
    "}\n",
    "\n",
    "# Function to plot the grouped bar graph with error bars\n",
    "def plot_grouped_bars_combined(filtered_df, ans_jgt, role=None):\n",
    "    # Ensure 'cot_type' ordering.\n",
    "    cot_order = ['cot1', 'cot2', 'cot0']\n",
    "    \n",
    "    # Map 'schema_name' to either \"Unconstrained\" or \"Structured Output\"\n",
    "    filtered_df['schema_group'] = filtered_df['schema_name'].apply(classify_schema)\n",
    "    \n",
    "    # Map temperature to readable labels\n",
    "    temp_mapping = {'temp0': 'temp=0.0', 'temp1': 'temp=1.0'}\n",
    "    filtered_df['temperature_label'] = filtered_df['temperature'].map(temp_mapping)\n",
    "    \n",
    "    # Combine 'schema_group' and 'temperature_label' to create 'group' variable\n",
    "    filtered_df['group'] = filtered_df['schema_group'] + ' (' + filtered_df['temperature_label'] + ')'\n",
    "    \n",
    "    # Calculate the error bars using accuracy standard deviation\n",
    "    filtered_df['yerr'] = filtered_df['accuracy_stdev']\n",
    "    \n",
    "    # Create a pivot table for plotting\n",
    "    pivot_df = filtered_df.pivot_table(\n",
    "        index='cot_type',\n",
    "        columns='group',\n",
    "        values=['accuracy_mean', 'yerr']\n",
    "    ).reindex(cot_order)\n",
    "    \n",
    "    # Get the number of CoT types and groups\n",
    "    num_cot_types = len(cot_order)\n",
    "    groups = pivot_df.columns.levels[1]\n",
    "    num_groups = len(groups)\n",
    "    \n",
    "    # Width settings for grouped bars\n",
    "    total_width = 0.8\n",
    "    bar_width = total_width / num_groups\n",
    "    x = np.arange(num_cot_types)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Define colors for each group\n",
    "    colors = {\n",
    "        'Unconstrained (temp=0.0)': 'tab:blue',\n",
    "        'Unconstrained (temp=1.0)': 'lightblue',\n",
    "        'Structured Output (temp=0.0)': 'tab:orange',\n",
    "        'Structured Output (temp=1.0)': 'navajowhite'  # light orange\n",
    "    }\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        means = pivot_df['accuracy_mean'][group].values\n",
    "        errors = pivot_df['yerr'][group].values\n",
    "        positions = x + i * bar_width - total_width/2 + bar_width/2\n",
    "        \n",
    "        bars = ax.bar(\n",
    "            positions,\n",
    "            means,\n",
    "            yerr=errors,\n",
    "            width=bar_width,\n",
    "            label=group,\n",
    "            capsize=5,\n",
    "            edgecolor='black',\n",
    "            color=colors.get(group, 'gray'),\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Add text annotations for each bar\n",
    "        ax.bar_label(bars, labels=[f'{mean:.3f}' for mean in means], padding=3, fontsize=10)\n",
    "    \n",
    "    # Set x-ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([cot_labels[label] for label in cot_order])\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Mean Accuracy')\n",
    "\n",
    "    # Get base title\n",
    "    if ans_jgt == \"ans\":\n",
    "        base_title = \"Direct Answering\"\n",
    "    else:\n",
    "        base_title = \"Proxy Answering\"\n",
    "\n",
    "    if role:\n",
    "        ax.set_title(f'{base_title}, Role: {role.capitalize()}')\n",
    "    else:\n",
    "        ax.set_title(f'{base_title}')\n",
    "    \n",
    "    ax.legend(title='')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Function to extract CoT type from 'prompt_template'\n",
    "def extract_cot_type(prompt_template):\n",
    "    return prompt_template.split('_')[-1]\n",
    "\n",
    "# Plotting for 'ans' (Direct Answering)\n",
    "filtered_df_ans = df_metrics[\n",
    "    df_metrics['prompt_template'].str.contains('ans') &\n",
    "    (df_metrics['role'] == 'no_role')\n",
    "].copy()\n",
    "\n",
    "# Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "filtered_df_ans['cot_type'] = filtered_df_ans['prompt_template'].apply(extract_cot_type)\n",
    "\n",
    "# Proceed to plot\n",
    "plot_grouped_bars_combined(filtered_df_ans, ans_jgt='ans')\n",
    "\n",
    "# Plotting for 'jgt' (Proxy Answering)\n",
    "for role in ['student', 'expert']:\n",
    "    filtered_df_jgt = df_metrics[\n",
    "        df_metrics['prompt_template'].str.contains('jgt') &\n",
    "        (df_metrics['role'] == role) &\n",
    "        (df_metrics['llm'] == LLM_TO_ANALYZE)\n",
    "    ].copy()\n",
    "    \n",
    "    # Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "    filtered_df_jgt['cot_type'] = filtered_df_jgt['prompt_template'].apply(extract_cot_type)\n",
    "    \n",
    "    # Proceed to plot\n",
    "    plot_grouped_bars_combined(filtered_df_jgt, ans_jgt='jgt', role=role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15758bf8-e7d8-4cd4-8981-8a5a2faf1be1",
   "metadata": {},
   "source": [
    "# Plotting Exponentially Weighted Consistency Metric (k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba26184-afa2-40bb-a3a9-1f63b26242b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to classify schema names as \"Unconstrained\" or \"Structured Output\"\n",
    "def classify_schema(schema_name):\n",
    "    if schema_name.lower() == 'no_schema':\n",
    "        return 'Unconstrained'\n",
    "    else:\n",
    "        return 'Structured Output'\n",
    "\n",
    "# Mapping for renaming CoT types\n",
    "cot_labels = {\n",
    "    'cot0': 'No CoT',\n",
    "    'cot1': 'CoT then Answer',\n",
    "    'cot2': 'Answer then CoT'\n",
    "}\n",
    "\n",
    "# Function to plot the grouped bar graph with error bars\n",
    "def plot_grouped_bars_combined(filtered_df, ans_jgt, role=None):\n",
    "    # Ensure 'cot_type' ordering.\n",
    "    cot_order = ['cot1', 'cot2', 'cot0']\n",
    "    \n",
    "    # Map 'schema_name' to either \"Unconstrained\" or \"Structured Output\"\n",
    "    filtered_df['schema_group'] = filtered_df['schema_name'].apply(classify_schema)\n",
    "    \n",
    "    # Map temperature to readable labels\n",
    "    temp_mapping = {'temp0': 'temp=0.0', 'temp1': 'temp=1.0'}\n",
    "    filtered_df['temperature_label'] = filtered_df['temperature'].map(temp_mapping)\n",
    "    \n",
    "    # Combine 'schema_group' and 'temperature_label' to create 'group' variable\n",
    "    filtered_df['group'] = filtered_df['schema_group'] + ' (' + filtered_df['temperature_label'] + ')'\n",
    "    \n",
    "    # Calculate the error bars using accuracy standard deviation\n",
    "    # filtered_df['yerr'] = filtered_df['accuracy_stdev']\n",
    "    \n",
    "    # Create a pivot table for plotting\n",
    "    pivot_df = filtered_df.pivot_table(\n",
    "        index='cot_type',\n",
    "        columns='group',\n",
    "        values=['exponential_consistency_weighted_metric_k2']\n",
    "    ).reindex(cot_order)\n",
    "    \n",
    "    # Get the number of CoT types and groups\n",
    "    num_cot_types = len(cot_order)\n",
    "    groups = pivot_df.columns.levels[1]\n",
    "    num_groups = len(groups)\n",
    "    \n",
    "    # Width settings for grouped bars\n",
    "    total_width = 0.8\n",
    "    bar_width = total_width / num_groups\n",
    "    x = np.arange(num_cot_types)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Define colors for each group\n",
    "    colors = {\n",
    "        'Unconstrained (temp=0.0)': 'tab:blue',\n",
    "        'Unconstrained (temp=1.0)': 'lightblue',\n",
    "        'Structured Output (temp=0.0)': 'tab:orange',\n",
    "        'Structured Output (temp=1.0)': 'navajowhite'  # light orange\n",
    "    }\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        means = pivot_df['exponential_consistency_weighted_metric_k2'][group].values\n",
    "        # errors = pivot_df['yerr'][group].values\n",
    "        positions = x + i * bar_width - total_width/2 + bar_width/2\n",
    "        \n",
    "        bars = ax.bar(\n",
    "            positions,\n",
    "            means,\n",
    "            # yerr=errors,\n",
    "            width=bar_width,\n",
    "            label=group,\n",
    "            capsize=5,\n",
    "            edgecolor='black',\n",
    "            color=colors.get(group, 'gray'),\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Add text annotations for each bar\n",
    "        ax.bar_label(bars, labels=[f'{mean:.3f}' for mean in means], padding=3, fontsize=10)\n",
    "    \n",
    "    # Set x-ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([cot_labels[label] for label in cot_order])\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Exponential Consistency Weighted Accuracy (k=2)')\n",
    "\n",
    "    # Get base title\n",
    "    if ans_jgt == \"ans\":\n",
    "        base_title = \"Direct Answering\"\n",
    "    else:\n",
    "        base_title = \"Proxy Answering\"\n",
    "\n",
    "    if role:\n",
    "        ax.set_title(f'{base_title}, Role: {role.capitalize()}')\n",
    "    else:\n",
    "        ax.set_title(f'{base_title}')\n",
    "    \n",
    "    ax.legend(title='')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Function to extract CoT type from 'prompt_template'\n",
    "def extract_cot_type(prompt_template):\n",
    "    return prompt_template.split('_')[-1]\n",
    "\n",
    "# Plotting for 'ans' (Direct Answering)\n",
    "filtered_df_ans = df_metrics[\n",
    "    df_metrics['prompt_template'].str.contains('ans') &\n",
    "    (df_metrics['role'] == 'no_role')\n",
    "].copy()\n",
    "\n",
    "# Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "filtered_df_ans['cot_type'] = filtered_df_ans['prompt_template'].apply(extract_cot_type)\n",
    "\n",
    "# Proceed to plot\n",
    "plot_grouped_bars_combined(filtered_df_ans, ans_jgt='ans')\n",
    "\n",
    "# Plotting for 'jgt' (Proxy Answering)\n",
    "for role in ['student', 'expert']:\n",
    "    filtered_df_jgt = df_metrics[\n",
    "        df_metrics['prompt_template'].str.contains('jgt') &\n",
    "        (df_metrics['role'] == role)\n",
    "    ].copy()\n",
    "    \n",
    "    # Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "    filtered_df_jgt['cot_type'] = filtered_df_jgt['prompt_template'].apply(extract_cot_type)\n",
    "    \n",
    "    # Proceed to plot\n",
    "    plot_grouped_bars_combined(filtered_df_jgt, ans_jgt='jgt', role=role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac33b8-741a-4a47-8805-244d8401091b",
   "metadata": {},
   "source": [
    "# Plotting Response Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1774c-4b1c-4015-90bd-9c4eb1739149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to classify schema names as \"Unconstrained\" or \"Structured Output\"\n",
    "def classify_schema(schema_name):\n",
    "    if schema_name.lower() == 'no_schema':\n",
    "        return 'Unconstrained'\n",
    "    else:\n",
    "        return 'Structured Output'\n",
    "\n",
    "# Mapping for renaming CoT types\n",
    "cot_labels = {\n",
    "    'cot0': 'No CoT',\n",
    "    'cot1': 'CoT then Answer',\n",
    "    'cot2': 'Answer then CoT'\n",
    "}\n",
    "\n",
    "# Function to plot the grouped bar graph with error bars\n",
    "def plot_grouped_bars_combined(filtered_df, ans_jgt, role=None):\n",
    "    # Ensure 'cot_type' ordering.\n",
    "    cot_order = ['cot1', 'cot2', 'cot0']\n",
    "    \n",
    "    # Map 'schema_name' to either \"Unconstrained\" or \"Structured Output\"\n",
    "    filtered_df['schema_group'] = filtered_df['schema_name'].apply(classify_schema)\n",
    "    \n",
    "    # Map temperature to readable labels\n",
    "    temp_mapping = {'temp0': 'temp=0.0', 'temp1': 'temp=1.0'}\n",
    "    filtered_df['temperature_label'] = filtered_df['temperature'].map(temp_mapping)\n",
    "    \n",
    "    # Combine 'schema_group' and 'temperature_label' to create 'group' variable\n",
    "    filtered_df['group'] = filtered_df['schema_group'] + ' (' + filtered_df['temperature_label'] + ')'\n",
    "    \n",
    "    # Calculate the error bars using accuracy standard deviation\n",
    "    filtered_df['yerr'] = filtered_df['response_length_stdev']\n",
    "    \n",
    "    # Create a pivot table for plotting\n",
    "    pivot_df = filtered_df.pivot_table(\n",
    "        index='cot_type',\n",
    "        columns='group',\n",
    "        values=['response_length_mean', 'yerr']\n",
    "    ).reindex(cot_order)\n",
    "    \n",
    "    # Get the number of CoT types and groups\n",
    "    num_cot_types = len(cot_order)\n",
    "    groups = pivot_df.columns.levels[1]\n",
    "    num_groups = len(groups)\n",
    "    \n",
    "    # Width settings for grouped bars\n",
    "    total_width = 0.8\n",
    "    bar_width = total_width / num_groups\n",
    "    x = np.arange(num_cot_types)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Define colors for each group\n",
    "    colors = {\n",
    "        'Unconstrained (temp=0.0)': 'tab:blue',\n",
    "        'Unconstrained (temp=1.0)': 'lightblue',\n",
    "        'Structured Output (temp=0.0)': 'tab:orange',\n",
    "        'Structured Output (temp=1.0)': 'navajowhite'  # light orange\n",
    "    }\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        means = pivot_df['response_length_mean'][group].values\n",
    "        errors = pivot_df['yerr'][group].values\n",
    "        positions = x + i * bar_width - total_width/2 + bar_width/2\n",
    "        \n",
    "        bars = ax.bar(\n",
    "            positions,\n",
    "            means,\n",
    "            yerr=errors,\n",
    "            width=bar_width,\n",
    "            label=group,\n",
    "            capsize=5,\n",
    "            edgecolor='black',\n",
    "            color=colors.get(group, 'gray'),\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Add text annotations for each bar\n",
    "        ax.bar_label(bars, labels=[f'{mean:.0f}' for mean in means], padding=3, fontsize=10)\n",
    "    \n",
    "    # Set x-ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([cot_labels[label] for label in cot_order])\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Mean Response Length')\n",
    "\n",
    "    # Get base title\n",
    "    if ans_jgt == \"ans\":\n",
    "        base_title = \"Direct Answering\"\n",
    "    else:\n",
    "        base_title = \"Proxy Answering\"\n",
    "\n",
    "    if role:\n",
    "        ax.set_title(f'{base_title}, Role: {role.capitalize()}')\n",
    "    else:\n",
    "        ax.set_title(f'{base_title}')\n",
    "    \n",
    "    ax.legend(title='')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Function to extract CoT type from 'prompt_template'\n",
    "def extract_cot_type(prompt_template):\n",
    "    return prompt_template.split('_')[-1]\n",
    "\n",
    "# Plotting for 'ans' (Direct Answering)\n",
    "filtered_df_ans = df_metrics[\n",
    "    df_metrics['prompt_template'].str.contains('ans') &\n",
    "    (df_metrics['role'] == 'no_role')\n",
    "].copy()\n",
    "\n",
    "# Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "filtered_df_ans['cot_type'] = filtered_df_ans['prompt_template'].apply(extract_cot_type)\n",
    "\n",
    "# Proceed to plot\n",
    "plot_grouped_bars_combined(filtered_df_ans, ans_jgt='ans')\n",
    "\n",
    "# Plotting for 'jgt' (Proxy Answering)\n",
    "for role in ['student', 'expert']:\n",
    "    filtered_df_jgt = df_metrics[\n",
    "        df_metrics['prompt_template'].str.contains('jgt') &\n",
    "        (df_metrics['role'] == role)\n",
    "    ].copy()\n",
    "    \n",
    "    # Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "    filtered_df_jgt['cot_type'] = filtered_df_jgt['prompt_template'].apply(extract_cot_type)\n",
    "    \n",
    "    # Proceed to plot\n",
    "    plot_grouped_bars_combined(filtered_df_jgt, ans_jgt='jgt', role=role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6614ac0-31d1-4036-9d57-3abd2375ac12",
   "metadata": {},
   "source": [
    "# Plotting the response length standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ea79d-37e9-4983-b65a-4ea400ef33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to classify schema names as \"Unconstrained\" or \"Structured Output\"\n",
    "def classify_schema(schema_name):\n",
    "    if schema_name.lower() == 'no_schema':\n",
    "        return 'Unconstrained'\n",
    "    else:\n",
    "        return 'Structured Output'\n",
    "\n",
    "# Mapping for renaming CoT types\n",
    "cot_labels = {\n",
    "    'cot0': 'No CoT',\n",
    "    'cot1': 'CoT then Answer',\n",
    "    'cot2': 'Answer then CoT'\n",
    "}\n",
    "\n",
    "# Function to plot the grouped bar graph with error bars\n",
    "def plot_grouped_bars_combined(filtered_df, ans_jgt, role=None):\n",
    "    # Ensure 'cot_type' ordering.\n",
    "    cot_order = ['cot1', 'cot2', 'cot0']\n",
    "    \n",
    "    # Map 'schema_name' to either \"Unconstrained\" or \"Structured Output\"\n",
    "    filtered_df['schema_group'] = filtered_df['schema_name'].apply(classify_schema)\n",
    "    \n",
    "    # Map temperature to readable labels\n",
    "    temp_mapping = {'temp0': 'temp=0.0', 'temp1': 'temp=1.0'}\n",
    "    filtered_df['temperature_label'] = filtered_df['temperature'].map(temp_mapping)\n",
    "    \n",
    "    # Combine 'schema_group' and 'temperature_label' to create 'group' variable\n",
    "    filtered_df['group'] = filtered_df['schema_group'] + ' (' + filtered_df['temperature_label'] + ')'\n",
    "    \n",
    "    # Calculate the error bars using accuracy standard deviation\n",
    "    filtered_df['yerr'] = filtered_df['response_length_stdev_stdev']\n",
    "    \n",
    "    # Create a pivot table for plotting\n",
    "    pivot_df = filtered_df.pivot_table(\n",
    "        index='cot_type',\n",
    "        columns='group',\n",
    "        values=['response_length_stdev_mean', 'yerr']\n",
    "    ).reindex(cot_order)\n",
    "    \n",
    "    # Get the number of CoT types and groups\n",
    "    num_cot_types = len(cot_order)\n",
    "    groups = pivot_df.columns.levels[1]\n",
    "    num_groups = len(groups)\n",
    "    \n",
    "    # Width settings for grouped bars\n",
    "    total_width = 0.8\n",
    "    bar_width = total_width / num_groups\n",
    "    x = np.arange(num_cot_types)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Define colors for each group\n",
    "    colors = {\n",
    "        'Unconstrained (temp=0.0)': 'tab:blue',\n",
    "        'Unconstrained (temp=1.0)': 'lightblue',\n",
    "        'Structured Output (temp=0.0)': 'tab:orange',\n",
    "        'Structured Output (temp=1.0)': 'navajowhite'  # light orange\n",
    "    }\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        means = pivot_df['response_length_stdev_mean'][group].values\n",
    "        errors = pivot_df['yerr'][group].values\n",
    "        positions = x + i * bar_width - total_width/2 + bar_width/2\n",
    "        \n",
    "        bars = ax.bar(\n",
    "            positions,\n",
    "            means,\n",
    "            yerr=errors,\n",
    "            width=bar_width,\n",
    "            label=group,\n",
    "            capsize=5,\n",
    "            edgecolor='black',\n",
    "            color=colors.get(group, 'gray'),\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Add text annotations for each bar\n",
    "        ax.bar_label(bars, labels=[f'{mean:.0f}' for mean in means], padding=3, fontsize=10)\n",
    "    \n",
    "    # Set x-ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([cot_labels[label] for label in cot_order])\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Response Length Stdev')\n",
    "\n",
    "    # Get base title\n",
    "    if ans_jgt == \"ans\":\n",
    "        base_title = \"Direct Answering\"\n",
    "    else:\n",
    "        base_title = \"Proxy Answering\"\n",
    "\n",
    "    if role:\n",
    "        ax.set_title(f'{base_title}, Role: {role.capitalize()}')\n",
    "    else:\n",
    "        ax.set_title(f'{base_title}')\n",
    "    \n",
    "    ax.legend(title='')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Function to extract CoT type from 'prompt_template'\n",
    "def extract_cot_type(prompt_template):\n",
    "    return prompt_template.split('_')[-1]\n",
    "\n",
    "# Plotting for 'ans' (Direct Answering)\n",
    "filtered_df_ans = df_metrics[\n",
    "    df_metrics['prompt_template'].str.contains('ans') &\n",
    "    (df_metrics['role'] == 'no_role')\n",
    "].copy()\n",
    "\n",
    "# Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "filtered_df_ans['cot_type'] = filtered_df_ans['prompt_template'].apply(extract_cot_type)\n",
    "\n",
    "# Proceed to plot\n",
    "plot_grouped_bars_combined(filtered_df_ans, ans_jgt='ans')\n",
    "\n",
    "# Plotting for 'jgt' (Proxy Answering)\n",
    "for role in ['student', 'expert']:\n",
    "    filtered_df_jgt = df_metrics[\n",
    "        df_metrics['prompt_template'].str.contains('jgt') &\n",
    "        (df_metrics['role'] == role)\n",
    "    ].copy()\n",
    "    \n",
    "    # Add CoT type as a separate column (cot0, cot1, cot2)\n",
    "    filtered_df_jgt['cot_type'] = filtered_df_jgt['prompt_template'].apply(extract_cot_type)\n",
    "    \n",
    "    # Proceed to plot\n",
    "    plot_grouped_bars_combined(filtered_df_jgt, ans_jgt='jgt', role=role)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
